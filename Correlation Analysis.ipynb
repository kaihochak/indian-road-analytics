{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Accident Hotspots Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.csv('Road.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Time',\n",
       " 'Day_of_week',\n",
       " 'Age_band_of_driver',\n",
       " 'Sex_of_driver',\n",
       " 'Educational_level',\n",
       " 'Vehicle_driver_relation',\n",
       " 'Driving_experience',\n",
       " 'Type_of_vehicle',\n",
       " 'Owner_of_vehicle',\n",
       " 'Service_year_of_vehicle',\n",
       " 'Defect_of_vehicle',\n",
       " 'Area_accident_occured',\n",
       " 'Lanes_or_Medians',\n",
       " 'Road_allignment',\n",
       " 'Types_of_Junction',\n",
       " 'Road_surface_type',\n",
       " 'Road_surface_conditions',\n",
       " 'Light_conditions',\n",
       " 'Weather_conditions',\n",
       " 'Type_of_collision',\n",
       " 'Number_of_vehicles_involved',\n",
       " 'Number_of_casualties',\n",
       " 'Vehicle_movement',\n",
       " 'Casualty_class',\n",
       " 'Sex_of_casualty',\n",
       " 'Age_band_of_casualty',\n",
       " 'Casualty_severity',\n",
       " 'Work_of_casuality',\n",
       " 'Fitness_of_casuality',\n",
       " 'Pedestrian_movement',\n",
       " 'Cause_of_accident',\n",
       " 'Accident_severity']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12316"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Analysis - Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "perform_chi_square_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from scipy.stats import chi2_contingency\n",
    "import pandas as pd\n",
    "\n",
    "def perform_chi_square_test(df, column1, column2):\n",
    "    \"\"\"\n",
    "    Perform a Chi-Square test on two categorical columns of a PySpark DataFrame.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): PySpark DataFrame.\n",
    "    column1 (str): Name of the first column.\n",
    "    column2 (str): Name of the second column.\n",
    "\n",
    "    Returns:\n",
    "    p value\n",
    "    \"\"\"\n",
    "    # Create the contingency table\n",
    "    contingency_table = df.groupBy(column1, column2).count()\n",
    "\n",
    "    # Convert to Pandas DataFrame for chi-square test\n",
    "    contingency_pd = contingency_table.toPandas()\n",
    "    contingency_pd_pivot = contingency_pd.pivot(index=column1, columns=column2, values='count').fillna(0)\n",
    "\n",
    "    # Perform the Chi-Square Test\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency_pd_pivot)\n",
    "    \n",
    "    # Return\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chi_square_test_on_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "def chi_square_test_on_features(df: DataFrame, target_column: str, significance_level: float) -> list:\n",
    "    \"\"\"\n",
    "    Filters the target column in a PySpark DataFrame, iterates over each feature column,\n",
    "    performs chi-square tests, plots the results, and returns the list of test results.\n",
    "\n",
    "    :param df: PySpark DataFrame to be used.\n",
    "    :param target_column: The target column for chi-square tests.\n",
    "    :param significance_level: The significance level for the chi-square test.\n",
    "    :return: List of tuples containing column names, test results, and p-values.\n",
    "    \"\"\"\n",
    "    # Filter the target column\n",
    "    df = df.filter(\n",
    "        df[target_column].isNotNull() & \n",
    "        (~df[target_column].isin(['unknown', 'Unknown', 'Other', 'other', 'na', 'normalNormal']))\n",
    "    )\n",
    "\n",
    "    test_results = []\n",
    "\n",
    "    # Iterate over each feature column\n",
    "    for column in df.columns:\n",
    "        if column != target_column:\n",
    "            # Filter the feature column\n",
    "            df_filtered = df.filter(\n",
    "                df[column].isNotNull() & \n",
    "                (~df[column].isin(['unknown', 'Unknown', 'Other', 'other', 'na', 'normalNormal']))\n",
    "            )\n",
    "\n",
    "            # Perform chi-square test (assuming perform_chi_square_test is defined)\n",
    "            p = perform_chi_square_test(df_filtered, column, target_column)\n",
    "\n",
    "            # Compare p-value with significance level and determine the result\n",
    "            result = 'Dependent (reject H0)' if p <= significance_level else 'Independent (H0 holds true)'\n",
    "\n",
    "            # Append column name, result, and p-value to the list\n",
    "            test_results.append((column, result, p))\n",
    "\n",
    "    # Plotting the results\n",
    "    plot_df = pd.DataFrame(test_results, columns=['Feature', 'Result', 'P-Value'])\n",
    "\n",
    "    # Set the figure size\n",
    "    plt.figure(figsize=(10, len(plot_df['Feature']) / 2))\n",
    "\n",
    "    # Horizontal bar chart\n",
    "    sns.barplot(x='P-Value', y='Feature', data=plot_df)\n",
    "    plt.title('P-Values of Features')\n",
    "    plt.show()\n",
    "\n",
    "    # Return the list of test results\n",
    "    return test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'perform_chi_square_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Call the chi_square_test_on_features function\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m test_results \u001b[38;5;241m=\u001b[39m \u001b[43mchi_square_test_on_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAccident_severity\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Create a Pandas DataFrame from the list for table display\u001b[39;00m\n\u001b[1;32m      8\u001b[0m results_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(test_results, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeature\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResult\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP-Value\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[5], line 34\u001b[0m, in \u001b[0;36mchi_square_test_on_features\u001b[0;34m(df, target_column, significance_level)\u001b[0m\n\u001b[1;32m     28\u001b[0m df_filtered \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mfilter(\n\u001b[1;32m     29\u001b[0m     df[column]\u001b[38;5;241m.\u001b[39misNotNull() \u001b[38;5;241m&\u001b[39m \n\u001b[1;32m     30\u001b[0m     (\u001b[38;5;241m~\u001b[39mdf[column]\u001b[38;5;241m.\u001b[39misin([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munknown\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnknown\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOther\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mother\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mna\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormalNormal\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Perform chi-square test (assuming perform_chi_square_test is defined)\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[43mperform_chi_square_test\u001b[49m(df_filtered, column, target_column)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Compare p-value with significance level and determine the result\u001b[39;00m\n\u001b[1;32m     37\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDependent (reject H0)\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m significance_level \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIndependent (H0 holds true)\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'perform_chi_square_test' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Call the chi_square_test_on_features function\n",
    "test_results = chi_square_test_on_features(df, 'Accident_severity', 0.05)\n",
    "\n",
    "# Create a Pandas DataFrame from the list for table display\n",
    "results_df = pd.DataFrame(test_results, columns=['Feature', 'Result', 'P-Value'])\n",
    "results_df = results_df.sort_values(by='P-Value', ascending=True)\n",
    "\n",
    "# Display the table\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sex_of_driver vs Accident_severity (Not statistically Significant)\n",
    "If a correlation is found, it might suggest that certain driver demographics are more prone to be involved in severe accidents in specific areas. This can lead to targeted safety programs or interventions focused on these demographics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accident_severity - Independent (H0 holds true)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "# Initialize Spark Session (if not already initialized)\n",
    "spark = SparkSession.builder.appName(\"DataTransform\").getOrCreate()\n",
    "\n",
    "# clean and run chi square test for all features\n",
    "clean_and_chiSqaure(df, 'Sex_of_driver', 'Accident_severity')\n",
    "\n",
    "# p_values = chi_square_test_on_features(df, 'Accident_severity')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Educational_level vs Accident_severity (>P, Weak Association)\n",
    "Correlation here could indicate the need for educational campaigns or driver safety programs tailored to different education levels, potentially reducing accident severity in specific regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Assuming 'df' is your PySpark DataFrame and 'Sex_of_driver' is the column of interest\n",
    "df_filter = df.filter(df['Educational_level'].isNotNull() & (df['Educational_level'] != 'unknown') & (df['Educational_level'] != 'Unknown'))\n",
    "df_filter = df_filter.filter(df_filter['Accident_severity'].isNotNull() & (df_filter['Accident_severity'] != 'unknown') & (df_filter['Accident_severity'] != 'Unknown'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from scipy.stats import chi2_contingency\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"ChiSquareTest\").getOrCreate()\n",
    "\n",
    "# Assuming df is your PySpark DataFrame\n",
    "# Create the contingency table\n",
    "contingency_table = df.groupBy('Educational_level', 'Accident_severity').count()\n",
    "\n",
    "# Convert to Pandas DataFrame for chi-square test\n",
    "contingency_pd = contingency_table.toPandas()\n",
    "contingency_pd_pivot = contingency_pd.pivot(index='Educational_level', columns='Accident_severity', values='count').fillna(0)\n",
    "\n",
    "# Perform the Chi-Square Test\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_pd_pivot)\n",
    "\n",
    "# Output the results\n",
    "print(chi2, p, dof, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day_of_week vs Type_of_collision (<P, but not related to accident frequency or location)\n",
    "Identifying days with higher frequencies of certain types of collisions can inform scheduling for increased road safety measures or traffic enforcement on those specific days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Assuming 'df' is your PySpark DataFrame and 'Sex_of_driver' is the column of interest\n",
    "df_filter = df.filter(df['Day_of_week'].isNotNull() & (df['Day_of_week'] != 'unknown') & (df['Day_of_week'] != 'Unknown'))\n",
    "df_filter = df_filter.filter(df_filter['Type_of_collision'].isNotNull() & (df_filter['Type_of_collision'] != 'Other'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from scipy.stats import chi2_contingency\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"ChiSquareTest\").getOrCreate()\n",
    "\n",
    "# Assuming df is your PySpark DataFrame\n",
    "# Create the contingency table\n",
    "contingency_table = df.groupBy('Day_of_week', 'Type_of_collision').count()\n",
    "\n",
    "# Convert to Pandas DataFrame for chi-square test\n",
    "contingency_pd = contingency_table.toPandas()\n",
    "contingency_pd_pivot = contingency_pd.pivot(index='Day_of_week', columns='Type_of_collision', values='count').fillna(0)\n",
    "\n",
    "# Perform the Chi-Square Test\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_pd_pivot)\n",
    "\n",
    "# Output the results\n",
    "print(chi2, p, dof, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Weather_conditions vs Accident_severity*\n",
    "This correlation can help in predicting accident severity based on weather conditions. It can guide the deployment of weather-specific road safety measures in areas prone to severe weather-related accidents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Assuming 'df' is your PySpark DataFrame and 'Sex_of_driver' is the column of interest\n",
    "df_filter = df.filter(df['Weather_conditions'].isNotNull() & (df['Weather_conditions'] != 'Other') & (df['Weather_conditions'] != 'Unknown'))\n",
    "df_filter = df_filter.filter(df_filter['Accident_severity'].isNotNull() & (df_filter['Accident_severity'] != 'unknown') & (df_filter['Accident_severity'] != 'Unknown'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from scipy.stats import chi2_contingency\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"ChiSquareTest\").getOrCreate()\n",
    "\n",
    "# Assuming df is your PySpark DataFrame\n",
    "# Create the contingency table\n",
    "contingency_table = df.groupBy('Weather_conditions', 'Accident_severity').count()\n",
    "\n",
    "# Convert to Pandas DataFrame for chi-square test\n",
    "contingency_pd = contingency_table.toPandas()\n",
    "contingency_pd_pivot = contingency_pd.pivot(index='Weather_conditions', columns='Accident_severity', values='count').fillna(0)\n",
    "\n",
    "# Perform the Chi-Square Test\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_pd_pivot)\n",
    "\n",
    "# Output the results\n",
    "print(chi2, p, dof, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Light_conditions vs Accident_severity*\n",
    "Understanding how light conditions affect accident severity can inform infrastructure improvements (like street lighting) in specific areas, and help in scheduling increased patrolling during high-risk light conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Assuming 'df' is your PySpark DataFrame and 'Sex_of_driver' is the column of interest\n",
    "df_filter = df\n",
    "df_filter = df_filter.filter(df_filter['Accident_severity'].isNotNull() & (df_filter['Accident_severity'] != 'unknown') & (df_filter['Accident_severity'] != 'Unknown'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from scipy.stats import chi2_contingency\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"ChiSquareTest\").getOrCreate()\n",
    "\n",
    "# Assuming df is your PySpark DataFrame\n",
    "# Create the contingency table\n",
    "contingency_table = df.groupBy('Light_conditions', 'Accident_severity').count()\n",
    "\n",
    "# Convert to Pandas DataFrame for chi-square test\n",
    "contingency_pd = contingency_table.toPandas()\n",
    "contingency_pd_pivot = contingency_pd.pivot(index='Light_conditions', columns='Accident_severity', values='count').fillna(0)\n",
    "\n",
    "# Perform the Chi-Square Test\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_pd_pivot)\n",
    "\n",
    "# Output the results\n",
    "print(chi2, p, dof, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Road_surface_conditions vs Accident_severity*\n",
    "Knowing how road conditions influence accident severity can guide maintenance and infrastructure development in specific geographic locations to improve road safety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Assuming 'df' is your PySpark DataFrame and 'Sex_of_driver' is the column of interest\n",
    "df_filter = df\n",
    "df_filter = df_filter.filter(df_filter['Accident_severity'].isNotNull() & (df_filter['Accident_severity'] != 'unknown') & (df_filter['Accident_severity'] != 'Unknown'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from scipy.stats import chi2_contingency\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"ChiSquareTest\").getOrCreate()\n",
    "\n",
    "# Assuming df is your PySpark DataFrame\n",
    "# Create the contingency table\n",
    "contingency_table = df.groupBy('Road_surface_conditions', 'Accident_severity').count()\n",
    "\n",
    "# Convert to Pandas DataFrame for chi-square test\n",
    "contingency_pd = contingency_table.toPandas()\n",
    "contingency_pd_pivot = contingency_pd.pivot(index='Road_surface_conditions', columns='Accident_severity', values='count').fillna(0)\n",
    "\n",
    "# Perform the Chi-Square Test\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_pd_pivot)\n",
    "\n",
    "# Output the results\n",
    "print(chi2, p, dof, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Type_of_vehicle vs Accident_severity*\n",
    "This correlation can inform targeted vehicle safety standards or advisories for certain vehicle types, especially in areas where these vehicles are involved in more severe accidents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'df' is your PySpark DataFrame and 'Sex_of_driver' is the column of interest\n",
    "df_filter = df.filter(df['Type_of_vehicle'].isNotNull() & (df['Type_of_vehicle'] != 'Other') & (df['Type_of_vehicle'] != 'Bajaj'))\n",
    "# Assuming df_filter is your PySpark DataFrame\n",
    "df_filter = df_filter.withColumn(\"Type_of_vehicle\", F.regexp_replace(F.col(\"Type_of_vehicle\"), r\"\\s\\(.*\\)\", \"\"))\n",
    "\n",
    "df_filter = df_filter.filter(df_filter['Accident_severity'].isNotNull() & (df_filter['Accident_severity'] != 'unknown') & (df_filter['Accident_severity'] != 'Unknown'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from scipy.stats import chi2_contingency\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"ChiSquareTest\").getOrCreate()\n",
    "\n",
    "# Assuming df is your PySpark DataFrame\n",
    "# Create the contingency table\n",
    "contingency_table = df.groupBy('Type_of_vehicle', 'Accident_severity').count()\n",
    "\n",
    "# Convert to Pandas DataFrame for chi-square test\n",
    "contingency_pd = contingency_table.toPandas()\n",
    "contingency_pd_pivot = contingency_pd.pivot(index='Type_of_vehicle', columns='Accident_severity', values='count').fillna(0)\n",
    "\n",
    "# Perform the Chi-Square Test\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_pd_pivot)\n",
    "\n",
    "# Output the results\n",
    "print(chi2, p, dof, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Area_accident_occurred vs Accident_severity*\n",
    "Identifying areas prone to severe accidents is directly related to your goal. This information can be used to focus road safety improvements, traffic regulation changes, or public awareness campaigns in these specific locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Assuming 'df' is your PySpark DataFrame and 'Sex_of_driver' is the column of interest\n",
    "df_filter = df.filter(df['Area_accident_occured'].isNotNull() & (df['Area_accident_occured'] != 'Other') & (df['Area_accident_occured'] != 'Unknown') & (df['Area_accident_occured'] != 'Rural village areasOffice areas'))\n",
    "df_filter = df_filter.filter(df_filter['Accident_severity'].isNotNull() & (df_filter['Accident_severity'] != 'unknown') & (df_filter['Accident_severity'] != 'Unknown'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from scipy.stats import chi2_contingency\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"ChiSquareTest\").getOrCreate()\n",
    "\n",
    "# Assuming df is your PySpark DataFrame\n",
    "# Create the contingency table\n",
    "contingency_table = df.groupBy('Area_accident_occured', 'Accident_severity').count()\n",
    "\n",
    "# Convert to Pandas DataFrame for chi-square test\n",
    "contingency_pd = contingency_table.toPandas()\n",
    "contingency_pd_pivot = contingency_pd.pivot(index='Area_accident_occured', columns='Accident_severity', values='count').fillna(0)\n",
    "\n",
    "# Perform the Chi-Square Test\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_pd_pivot)\n",
    "\n",
    "# Output the results\n",
    "print(chi2, p, dof, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Analysis - Visual Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap for Temporal Pattern\n",
    "heatmaps for time-related data like \n",
    "\n",
    "*'Day_of_week' vs 'accident frequency'*\n",
    "\n",
    "*'Hour'vs 'accident frequency'*\n",
    "\n",
    "time of an accident is a significant factor and should definitely be included in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import hour, minute\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = df.withColumn('Hour', hour(df['Time']))\n",
    "df = df.withColumn('Minute', minute(df['Time']))\n",
    "\n",
    "df_filter = df.filter(df['Day_of_week'].isNotNull())\n",
    "df_filter = df_filter.filter(df_filter['Hour'].isNotNull())\n",
    "df_filter = df_filter.filter(df_filter['Accident_severity'].isNotNull() & (df_filter['Accident_severity'] != 'unknown') & (df_filter['Accident_severity'] != 'Unknown'))\n",
    "\n",
    "# Get distinct values for 'Day_of_week' and 'Area_accident_occured'\n",
    "day_of_week_unique = df_filter.select('Day_of_week').distinct()\n",
    "area_accident_occured_unique = df_filter.select('Area_accident_occured').distinct()\n",
    "\n",
    "# Creating a crosstab (contingency table) in PySpark\n",
    "contingency_table_df = df_filter.crosstab('Day_of_week', 'Area_accident_occured')\n",
    "\n",
    "# Convert the PySpark DataFrame to Pandas DataFrame for plotting\n",
    "contingency_table_pd = contingency_table_df.toPandas()\n",
    "\n",
    "# Set an appropriate index for the heatmap\n",
    "contingency_table_pd = contingency_table_pd.set_index('Day_of_week_Area_accident_occured')\n",
    "\n",
    "# Plotting the heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(contingency_table_pd, annot=True, fmt='d', cmap='viridis')\n",
    "plt.title('Correlation between Area of Accident Occurrence and Day of the Week')\n",
    "plt.ylabel('Day of the Week')\n",
    "plt.xlabel('Area of Accident Occurrence')\n",
    "plt.show()\n",
    "\n",
    "# Get distinct values for 'Day_of_week' and 'Area_accident_occured'\n",
    "Hour_unique = df_filter.select('Hour').distinct()\n",
    "area_accident_occured_unique = df_filter.select('Area_accident_occured').distinct()\n",
    "\n",
    "# Creating a crosstab (contingency table) in PySpark\n",
    "contingency_table_df = df_filter.crosstab('Hour', 'Area_accident_occured')\n",
    "\n",
    "# Convert the PySpark DataFrame to Pandas DataFrame for plotting\n",
    "contingency_table_pd = contingency_table_df.toPandas()\n",
    "\n",
    "# Set an appropriate index for the heatmap\n",
    "contingency_table_pd = contingency_table_pd.set_index('Hour_Area_accident_occured')\n",
    "\n",
    "# Plotting the heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(contingency_table_pd, annot=True, fmt='d', cmap='viridis')\n",
    "plt.title('Correlation between Area of Accident Occurrence and Hours')\n",
    "plt.ylabel('Hour')\n",
    "plt.xlabel('Area of Accident Occurrence')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bar Charts for Categorical Data Analysis\n",
    "\n",
    "*'Weather_conditions' vs 'count of accidents'*: even if accidents are less frequent in adverse weather conditions, they might still be more severe, so this feature should be included in the model\n",
    "\n",
    "*'Road_surface_conditions' vs 'count of accidents'*:  the severity and the risk of accidents might increase with adverse road conditions\n",
    "\n",
    "*'Light_conditions' vs 'count of accidents'*: show a strong correlation with the number of accidents\n",
    "\n",
    "*'Type_of_vehicle' vs 'count of accidents'*: This feature can help in understanding if certain vehicle types are more prone to accidents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Assuming 'df' is your PySpark DataFrame and 'Sex_of_driver' is the column of interest\n",
    "df_filter = df.filter(df['Weather_conditions'].isNotNull() & (df['Weather_conditions'] != 'Other') & (df['Weather_conditions'] != 'Unknown'))\n",
    "df_filter = df_filter.filter(df_filter['Road_surface_conditions'].isNotNull())\n",
    "df_filter = df_filter.filter(df_filter['Light_conditions'].isNotNull())\n",
    "df_filter = df_filter.filter(df_filter['Type_of_vehicle'].isNotNull() & (df['Type_of_vehicle'] != 'Other') & (df['Type_of_vehicle'] != 'Bajaj'))\n",
    "df_filter = df_filter.withColumn(\"Type_of_vehicle\", F.regexp_replace(F.col(\"Type_of_vehicle\"), r\"\\s\\(.*\\)\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming df_filter is your PySpark DataFrame\n",
    "# Group by 'Weather_conditions' and count\n",
    "accident_counts = df_filter.groupBy('Weather_conditions').count()\n",
    "\n",
    "# Convert to Pandas DataFrame for plotting\n",
    "accident_counts_pd = accident_counts.toPandas()\n",
    "\n",
    "# Sort the DataFrame for better visualization\n",
    "accident_counts_pd = accident_counts_pd.sort_values(by='count', ascending=False)\n",
    "\n",
    "# Creating the bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Weather_conditions', y='count', data=accident_counts_pd)\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title('Count of Accidents by Weather Conditions')\n",
    "plt.xlabel('Weather Conditions')\n",
    "plt.ylabel('Number of Accidents')\n",
    "plt.xticks(rotation=45)  # Rotating the x labels for better readability\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "# Group by 'Road_surface_conditions' and count\n",
    "road_surface_counts = df_filter.groupBy('Road_surface_conditions').count()\n",
    "\n",
    "# Convert to Pandas DataFrame for plotting\n",
    "road_surface_counts_pd = road_surface_counts.toPandas().sort_values(by='count', ascending=False)\n",
    "\n",
    "# Creating the bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Road_surface_conditions', y='count', data=road_surface_counts_pd)\n",
    "plt.title('Count of Accidents by Road Surface Conditions')\n",
    "plt.xlabel('Road Surface Conditions')\n",
    "plt.ylabel('Number of Accidents')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Group by 'Light_conditions' and count\n",
    "light_conditions_counts = df_filter.groupBy('Light_conditions').count()\n",
    "\n",
    "# Convert to Pandas DataFrame for plotting\n",
    "light_conditions_counts_pd = light_conditions_counts.toPandas().sort_values(by='count', ascending=False)\n",
    "\n",
    "# Creating the bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Light_conditions', y='count', data=light_conditions_counts_pd)\n",
    "plt.title('Count of Accidents by Light Conditions')\n",
    "plt.xlabel('Light Conditions')\n",
    "plt.ylabel('Number of Accidents')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Group by 'Type_of_vehicle' and count\n",
    "type_of_vehicle_counts = df_filter.groupBy('Type_of_vehicle').count()\n",
    "\n",
    "# Convert to Pandas DataFrame for plotting\n",
    "type_of_vehicle_counts_pd = type_of_vehicle_counts.toPandas().sort_values(by='count', ascending=False)\n",
    "\n",
    "# Creating the bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Type_of_vehicle', y='count', data=type_of_vehicle_counts_pd)\n",
    "plt.title('Count of Accidents by Type of Vehicle')\n",
    "plt.xlabel('Type of Vehicle')\n",
    "plt.ylabel('Number of Accidents')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m df_filter \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHour\u001b[39m\u001b[38;5;124m'\u001b[39m, hour(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m      6\u001b[0m df_filter \u001b[38;5;241m=\u001b[39m df_filter\u001b[38;5;241m.\u001b[39mfilter(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHour\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misNotNull())\n\u001b[1;32m      7\u001b[0m df_filter \u001b[38;5;241m=\u001b[39m df_filter\u001b[38;5;241m.\u001b[39mfilter(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDay_of_week\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misNotNull())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import hour\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df_filter = df.withColumn('Hour', hour(df['Time']))\n",
    "df_filter = df_filter.filter(df['Hour'].isNotNull())\n",
    "df_filter = df_filter.filter(df['Day_of_week'].isNotNull())\n",
    "\n",
    "# Group the data by 'Day_of_week' and 'Hour' and count the number of accidents\n",
    "accidents_by_day_hour = df_filter.groupBy('Day_of_week', 'Hour').count()\n",
    "\n",
    "# Convert to Pandas DataFrame for plotting\n",
    "accidents_by_day_hour_pd = accidents_by_day_hour.toPandas()\n",
    "\n",
    "# Pivot the data for better visualization\n",
    "accidents_pivot = accidents_by_day_hour_pd.pivot(index='Hour', columns='Day_of_week', values='count').fillna(0)\n",
    "\n",
    "# Plotting the data\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(accidents_pivot, annot=False, fmt=\".0f\", linewidths=.5, cmap='viridis')\n",
    "plt.title('Distribution of Accidents by Day of Week and Hour')\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('Hour of Day')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

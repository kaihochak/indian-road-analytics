{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/09 01:05:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Accident Hotspots Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.csv('Road.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting String to TimeStamp\n",
    "\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "df = df.withColumn('Time', to_timestamp(df['Time'], 'yyyy-MM-dd HH:mm:ss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Hour and Minutes From Time Field\n",
    "\n",
    "from pyspark.sql.functions import hour, minute\n",
    "\n",
    "df = df.withColumn('Hour', hour(df['Time']))\n",
    "df = df.withColumn('Minute', minute(df['Time']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling invalid/null Quantities from Hour and Minute\n",
    "\n",
    "from pyspark.sql.functions import hour, minute\n",
    "\n",
    "cols_to_keep = ['Time','Day_of_week','Area_accident_occured','Number_of_casualties','Accident_severity']\n",
    "df = df.select(*cols_to_keep)\n",
    "df = df.filter(df['Time'].isNotNull())\n",
    "\n",
    "df = df.withColumn('Hour', hour(df['Time']))\n",
    "df = df.withColumn('Minute', minute(df['Time']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+---------------------+--------------------+-----------------+----+------+\n",
      "|               Time|Day_of_week|Area_accident_occured|Number_of_casualties|Accident_severity|Hour|Minute|\n",
      "+-------------------+-----------+---------------------+--------------------+-----------------+----+------+\n",
      "|2023-12-09 17:02:00|     Monday|    Residential areas|                   2|    Slight Injury|  17|     2|\n",
      "|2023-12-09 17:02:00|     Monday|         Office areas|                   2|    Slight Injury|  17|     2|\n",
      "|2023-12-09 17:02:00|     Monday|   Recreational areas|                   2|   Serious Injury|  17|     2|\n",
      "|2023-12-09 01:06:00|     Sunday|         Office areas|                   2|    Slight Injury|   1|     6|\n",
      "|2023-12-09 01:06:00|     Sunday|     Industrial areas|                   2|    Slight Injury|   1|     6|\n",
      "|2023-12-09 14:15:00|     Friday|                 NULL|                   1|    Slight Injury|  14|    15|\n",
      "|2023-12-09 17:30:00|  Wednesday|    Residential areas|                   1|    Slight Injury|  17|    30|\n",
      "|2023-12-09 17:20:00|     Friday|    Residential areas|                   1|    Slight Injury|  17|    20|\n",
      "|2023-12-09 17:20:00|     Friday|     Industrial areas|                   1|    Slight Injury|  17|    20|\n",
      "|2023-12-09 17:20:00|     Friday|    Residential areas|                   1|   Serious Injury|  17|    20|\n",
      "|2023-12-09 14:40:00|   Saturday|    Residential areas|                   1|   Serious Injury|  14|    40|\n",
      "|2023-12-09 14:40:00|   Saturday|         Office areas|                   1|   Serious Injury|  14|    40|\n",
      "|2023-12-09 17:45:00|   Thursday|         Office areas|                   2|    Slight Injury|  17|    45|\n",
      "|2023-12-09 17:45:00|   Thursday|         Office areas|                   2|    Slight Injury|  17|    45|\n",
      "|2023-12-09 17:45:00|   Thursday|         Office areas|                   2|   Serious Injury|  17|    45|\n",
      "|2023-12-09 22:45:00|     Monday|         Office areas|                   3|   Serious Injury|  22|    45|\n",
      "|2023-12-09 22:45:00|     Monday|         Office areas|                   3|    Slight Injury|  22|    45|\n",
      "|2023-12-09 22:45:00|     Monday|         Office areas|                   3|   Serious Injury|  22|    45|\n",
      "|2023-12-09 22:45:00|     Monday|    Residential areas|                   3|    Slight Injury|  22|    45|\n",
      "|2023-12-09 08:20:00|    Tuesday|    Residential areas|                   1|   Serious Injury|   8|    20|\n",
      "+-------------------+-----------+---------------------+--------------------+-----------------+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Day_of_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Day_of_week to numberical format\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "\n",
    "# Create a user-defined function for converting days to numbers\n",
    "days_dict = {'Monday': 1, 'Tuesday': 2, 'Wednesday': 3, 'Thursday': 4, 'Friday': 5, 'Saturday': 6, 'Sunday': 7}\n",
    "convert_day_udf = udf(lambda day: days_dict.get(day, 0), IntegerType())\n",
    "\n",
    "# Apply the UDF to the 'Day_of_week' column\n",
    "df = df.withColumn('Day_of_week_numeric', convert_day_udf(df['Day_of_week']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+---------------------+--------------------+-----------------+----+------+-------------------+\n",
      "|               Time|Day_of_week|Area_accident_occured|Number_of_casualties|Accident_severity|Hour|Minute|Day_of_week_numeric|\n",
      "+-------------------+-----------+---------------------+--------------------+-----------------+----+------+-------------------+\n",
      "|2023-12-08 17:02:00|     Monday|    Residential areas|                   2|    Slight Injury|  17|     2|                  1|\n",
      "|2023-12-08 17:02:00|     Monday|         Office areas|                   2|    Slight Injury|  17|     2|                  1|\n",
      "|2023-12-08 17:02:00|     Monday|   Recreational areas|                   2|   Serious Injury|  17|     2|                  1|\n",
      "|2023-12-08 01:06:00|     Sunday|         Office areas|                   2|    Slight Injury|   1|     6|                  7|\n",
      "|2023-12-08 01:06:00|     Sunday|     Industrial areas|                   2|    Slight Injury|   1|     6|                  7|\n",
      "|2023-12-08 14:15:00|     Friday|                 NULL|                   1|    Slight Injury|  14|    15|                  5|\n",
      "|2023-12-08 17:30:00|  Wednesday|    Residential areas|                   1|    Slight Injury|  17|    30|                  3|\n",
      "|2023-12-08 17:20:00|     Friday|    Residential areas|                   1|    Slight Injury|  17|    20|                  5|\n",
      "|2023-12-08 17:20:00|     Friday|     Industrial areas|                   1|    Slight Injury|  17|    20|                  5|\n",
      "|2023-12-08 17:20:00|     Friday|    Residential areas|                   1|   Serious Injury|  17|    20|                  5|\n",
      "|2023-12-08 14:40:00|   Saturday|    Residential areas|                   1|   Serious Injury|  14|    40|                  6|\n",
      "|2023-12-08 14:40:00|   Saturday|         Office areas|                   1|   Serious Injury|  14|    40|                  6|\n",
      "|2023-12-08 17:45:00|   Thursday|         Office areas|                   2|    Slight Injury|  17|    45|                  4|\n",
      "|2023-12-08 17:45:00|   Thursday|         Office areas|                   2|    Slight Injury|  17|    45|                  4|\n",
      "|2023-12-08 17:45:00|   Thursday|         Office areas|                   2|   Serious Injury|  17|    45|                  4|\n",
      "|2023-12-08 22:45:00|     Monday|         Office areas|                   3|   Serious Injury|  22|    45|                  1|\n",
      "|2023-12-08 22:45:00|     Monday|         Office areas|                   3|    Slight Injury|  22|    45|                  1|\n",
      "|2023-12-08 22:45:00|     Monday|         Office areas|                   3|   Serious Injury|  22|    45|                  1|\n",
      "|2023-12-08 22:45:00|     Monday|    Residential areas|                   3|    Slight Injury|  22|    45|                  1|\n",
      "|2023-12-08 08:20:00|    Tuesday|    Residential areas|                   1|   Serious Injury|   8|    20|                  2|\n",
      "+-------------------+-----------+---------------------+--------------------+-----------------+----+------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Age_band_of_driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "len() of unsized object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/kai/Desktop/study/2023_fall/seng550/project/Q1.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kai/Desktop/study/2023_fall/seng550/project/Q1.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m encoder \u001b[39m=\u001b[39m OneHotEncoder(sparse\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kai/Desktop/study/2023_fall/seng550/project/Q1.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Applying one-hot encoding\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/kai/Desktop/study/2023_fall/seng550/project/Q1.ipynb#X13sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m age_band_encoded \u001b[39m=\u001b[39m encoder\u001b[39m.\u001b[39;49mfit_transform(age_band)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kai/Desktop/study/2023_fall/seng550/project/Q1.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Converting the encoded data into a DataFrame\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kai/Desktop/study/2023_fall/seng550/project/Q1.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m age_band_encoded_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(age_band_encoded, columns\u001b[39m=\u001b[39mencoder\u001b[39m.\u001b[39mget_feature_names_out([\u001b[39m'\u001b[39m\u001b[39mAge_band_of_driver\u001b[39m\u001b[39m'\u001b[39m]))\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sklearn/utils/_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    156\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 157\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    158\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    159\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    160\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[1;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    162\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    163\u001b[0m         )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sklearn/base.py:916\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[39m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[39m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[1;32m    914\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    915\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 916\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[1;32m    917\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    918\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    919\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:985\u001b[0m, in \u001b[0;36mOneHotEncoder.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    975\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    976\u001b[0m         (\n\u001b[1;32m    977\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`sparse` was renamed to `sparse_output` in version 1.2 and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    981\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    982\u001b[0m     )\n\u001b[1;32m    983\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msparse_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msparse\n\u001b[0;32m--> 985\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(\n\u001b[1;32m    986\u001b[0m     X,\n\u001b[1;32m    987\u001b[0m     handle_unknown\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle_unknown,\n\u001b[1;32m    988\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    989\u001b[0m )\n\u001b[1;32m    990\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_drop_idx()\n\u001b[1;32m    991\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_features_outs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_n_features_outs()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:77\u001b[0m, in \u001b[0;36m_BaseEncoder._fit\u001b[0;34m(self, X, handle_unknown, force_all_finite, return_counts, return_and_ignore_missing_for_infrequent)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_infrequent_enabled()\n\u001b[1;32m     76\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_n_features(X, reset\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 77\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_feature_names(X, reset\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     78\u001b[0m X_list, n_samples, n_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_X(\n\u001b[1;32m     79\u001b[0m     X, force_all_finite\u001b[39m=\u001b[39mforce_all_finite\n\u001b[1;32m     80\u001b[0m )\n\u001b[1;32m     81\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_ \u001b[39m=\u001b[39m n_features\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sklearn/base.py:441\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Set or check the `feature_names_in_` attribute.\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \n\u001b[1;32m    423\u001b[0m \u001b[39m.. versionadded:: 1.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[39m       should set `reset=False`.\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[39mif\u001b[39;00m reset:\n\u001b[0;32m--> 441\u001b[0m     feature_names_in \u001b[39m=\u001b[39m _get_feature_names(X)\n\u001b[1;32m    442\u001b[0m     \u001b[39mif\u001b[39;00m feature_names_in \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    443\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_names_in_ \u001b[39m=\u001b[39m feature_names_in\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sklearn/utils/validation.py:2013\u001b[0m, in \u001b[0;36m_get_feature_names\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m   2010\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(X, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   2011\u001b[0m     feature_names \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(X\u001b[39m.\u001b[39mcolumns, dtype\u001b[39m=\u001b[39m\u001b[39mobject\u001b[39m)\n\u001b[0;32m-> 2013\u001b[0m \u001b[39mif\u001b[39;00m feature_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39;49m(feature_names) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   2014\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   2016\u001b[0m types \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(t\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mset\u001b[39m(\u001b[39mtype\u001b[39m(v) \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m feature_names))\n",
      "\u001b[0;31mTypeError\u001b[0m: len() of unsized object"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Selecting the 'Age_band_of_driver' column\n",
    "age_band = df['Age_band_of_driver']\n",
    "\n",
    "# Creating the OneHotEncoder object\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Applying one-hot encoding\n",
    "age_band_encoded = encoder.fit_transform(age_band)\n",
    "\n",
    "# Converting the encoded data into a DataFrame\n",
    "age_band_encoded_df = pd.DataFrame(age_band_encoded, columns=encoder.get_feature_names_out(['Age_band_of_driver']))\n",
    "\n",
    "# Displaying the first few rows of the encoded DataFrame\n",
    "age_band_encoded_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Light_conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------------+------------------------+\n",
      "|    Light_conditions|Light_conditions_Index|Light_conditions_Encoded|\n",
      "+--------------------+----------------------+------------------------+\n",
      "|            Daylight|                   0.0|           (3,[0],[1.0])|\n",
      "|            Daylight|                   0.0|           (3,[0],[1.0])|\n",
      "|            Daylight|                   0.0|           (3,[0],[1.0])|\n",
      "|Darkness - lights...|                   1.0|           (3,[1],[1.0])|\n",
      "|Darkness - lights...|                   1.0|           (3,[1],[1.0])|\n",
      "|            Daylight|                   0.0|           (3,[0],[1.0])|\n",
      "|            Daylight|                   0.0|           (3,[0],[1.0])|\n",
      "|            Daylight|                   0.0|           (3,[0],[1.0])|\n",
      "|            Daylight|                   0.0|           (3,[0],[1.0])|\n",
      "|            Daylight|                   0.0|           (3,[0],[1.0])|\n",
      "|            Daylight|                   0.0|           (3,[0],[1.0])|\n",
      "|            Daylight|                   0.0|           (3,[0],[1.0])|\n",
      "|            Daylight|                   0.0|           (3,[0],[1.0])|\n",
      "|            Daylight|                   0.0|           (3,[0],[1.0])|\n",
      "|            Daylight|                   0.0|           (3,[0],[1.0])|\n",
      "|Darkness - lights...|                   1.0|           (3,[1],[1.0])|\n",
      "|Darkness - lights...|                   1.0|           (3,[1],[1.0])|\n",
      "|Darkness - lights...|                   1.0|           (3,[1],[1.0])|\n",
      "|Darkness - lights...|                   1.0|           (3,[1],[1.0])|\n",
      "|            Daylight|                   0.0|           (3,[0],[1.0])|\n",
      "+--------------------+----------------------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Read in the data  \n",
    "df = spark.read.csv('Road.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Filter out rows with null or 'Unknown' values in 'Weather_conditions'\n",
    "df_filtered = df.filter((df.Weather_conditions.isNotNull()) & (df.Weather_conditions != 'Unknown'))\n",
    "\n",
    "# StringIndexer to convert the 'Light_conditions' column to numeric indices\n",
    "stringIndexer = StringIndexer(inputCol=\"Light_conditions\", outputCol=\"Light_conditions_Index\")\n",
    "\n",
    "# Fit the indexer to the data and transform it\n",
    "indexed = stringIndexer.fit(df).transform(df_filtered)\n",
    "\n",
    "# OneHotEncoder to convert indexed numbers to one-hot encoded values\n",
    "encoder = OneHotEncoder(inputCols=[\"Light_conditions_Index\"], outputCols=[\"Light_conditions_Encoded\"])\n",
    "\n",
    "# Fit and transform\n",
    "encoded = encoder.fit(indexed).transform(indexed)\n",
    "\n",
    "# Show the results\n",
    "encoded.select(\"Light_conditions\", \"Light_conditions_Index\", \"Light_conditions_Encoded\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Weather_conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/09 02:32:12 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------------+--------------------------+\n",
      "|Weather_conditions|Weather_conditions_Index|Weather_conditions_Encoded|\n",
      "+------------------+------------------------+--------------------------+\n",
      "|            Normal|                     0.0|             (8,[0],[1.0])|\n",
      "|            Normal|                     0.0|             (8,[0],[1.0])|\n",
      "|            Normal|                     0.0|             (8,[0],[1.0])|\n",
      "|            Normal|                     0.0|             (8,[0],[1.0])|\n",
      "|            Normal|                     0.0|             (8,[0],[1.0])|\n",
      "|            Normal|                     0.0|             (8,[0],[1.0])|\n",
      "|            Normal|                     0.0|             (8,[0],[1.0])|\n",
      "|            Normal|                     0.0|             (8,[0],[1.0])|\n",
      "|            Normal|                     0.0|             (8,[0],[1.0])|\n",
      "|            Normal|                     0.0|             (8,[0],[1.0])|\n",
      "|            Normal|                     0.0|             (8,[0],[1.0])|\n",
      "|            Normal|                     0.0|             (8,[0],[1.0])|\n",
      "|            Normal|                     0.0|             (8,[0],[1.0])|\n",
      "|            Normal|                     0.0|             (8,[0],[1.0])|\n",
      "|            Normal|                     0.0|             (8,[0],[1.0])|\n",
      "|           Raining|                     1.0|             (8,[1],[1.0])|\n",
      "|           Raining|                     1.0|             (8,[1],[1.0])|\n",
      "|           Raining|                     1.0|             (8,[1],[1.0])|\n",
      "|           Raining|                     1.0|             (8,[1],[1.0])|\n",
      "|            Normal|                     0.0|             (8,[0],[1.0])|\n",
      "+------------------+------------------------+--------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"DataCleaning\").getOrCreate()\n",
    "\n",
    "# Read in the data  from csv file\n",
    "df = spark.read.csv('Road.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Filter out rows with null or 'Unknown' values in 'Weather_conditions'\n",
    "df_filtered = df.filter((df.Weather_conditions.isNotNull()) & (df.Weather_conditions != 'Unknown'))\n",
    "\n",
    "# StringIndexer to convert the 'Weather_conditions' column to numeric indices\n",
    "stringIndexer = StringIndexer(inputCol=\"Weather_conditions\", outputCol=\"Weather_conditions_Index\")\n",
    "\n",
    "# Fit the indexer to the data and transform it\n",
    "indexed = stringIndexer.fit(df).transform(df_filtered)\n",
    "\n",
    "# OneHotEncoder to convert indexed numbers to one-hot encoded values\n",
    "encoder = OneHotEncoder(inputCols=[\"Weather_conditions_Index\"], outputCols=[\"Weather_conditions_Encoded\"])\n",
    "\n",
    "# Fit and transform\n",
    "encoded = encoder.fit(indexed).transform(indexed)\n",
    "\n",
    "# Show the results\n",
    "encoded.select(\"Weather_conditions\", \"Weather_conditions_Index\", \"Weather_conditions_Encoded\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Type_of_collision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------------+-------------------------+\n",
      "|   Type_of_collision|Type_of_collision_Index|Type_of_collision_Encoded|\n",
      "+--------------------+-----------------------+-------------------------+\n",
      "|Collision with ro...|                    5.0|            (9,[5],[1.0])|\n",
      "|Vehicle with vehi...|                    0.0|            (9,[0],[1.0])|\n",
      "|Collision with ro...|                    1.0|            (9,[1],[1.0])|\n",
      "|Vehicle with vehi...|                    0.0|            (9,[0],[1.0])|\n",
      "|Vehicle with vehi...|                    0.0|            (9,[0],[1.0])|\n",
      "|Vehicle with vehi...|                    0.0|            (9,[0],[1.0])|\n",
      "|Vehicle with vehi...|                    0.0|            (9,[0],[1.0])|\n",
      "|Vehicle with vehi...|                    0.0|            (9,[0],[1.0])|\n",
      "|Collision with ro...|                    5.0|            (9,[5],[1.0])|\n",
      "|Collision with ro...|                    5.0|            (9,[5],[1.0])|\n",
      "|Collision with an...|                    4.0|            (9,[4],[1.0])|\n",
      "|Collision with an...|                    4.0|            (9,[4],[1.0])|\n",
      "|Collision with an...|                    4.0|            (9,[4],[1.0])|\n",
      "|Collision with an...|                    4.0|            (9,[4],[1.0])|\n",
      "|Collision with an...|                    4.0|            (9,[4],[1.0])|\n",
      "|Collision with an...|                    4.0|            (9,[4],[1.0])|\n",
      "|Collision with an...|                    4.0|            (9,[4],[1.0])|\n",
      "|Collision with an...|                    4.0|            (9,[4],[1.0])|\n",
      "|Collision with an...|                    4.0|            (9,[4],[1.0])|\n",
      "|Collision with an...|                    4.0|            (9,[4],[1.0])|\n",
      "+--------------------+-----------------------+-------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"DataCleaning\").getOrCreate()\n",
    "\n",
    "# Read in the data  \n",
    "df = spark.read.csv('Road.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Filter out rows with null or 'Unknown' values in 'Weather_conditions'\n",
    "df_filtered = df.filter((df.Type_of_collision.isNotNull()) & (df.Type_of_collision != 'Unknown'))\n",
    "\n",
    "# StringIndexer to convert the 'Type_of_collision' column to numeric indices\n",
    "stringIndexer = StringIndexer(inputCol=\"Type_of_collision\", outputCol=\"Type_of_collision_Index\")\n",
    "\n",
    "# Fit the indexer to the data and transform it\n",
    "indexed = stringIndexer.fit(df).transform(df_filtered)\n",
    "\n",
    "# OneHotEncoder to convert indexed numbers to one-hot encoded values\n",
    "encoder = OneHotEncoder(inputCols=[\"Type_of_collision_Index\"], outputCols=[\"Type_of_collision_Encoded\"])\n",
    "\n",
    "# Fit and transform\n",
    "encoded = encoder.fit(indexed).transform(indexed)\n",
    "\n",
    "# Show the results\n",
    "encoded.select(\"Type_of_collision\", \"Type_of_collision_Index\", \"Type_of_collision_Encoded\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Number_of_vehicles_involved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+----------------------------------+\n",
      "|Number_of_vehicles_involved|Number_of_vehicles_involved_scaled|\n",
      "+---------------------------+----------------------------------+\n",
      "|                          2|              [0.16666666666666...|\n",
      "|                          2|              [0.16666666666666...|\n",
      "|                          2|              [0.16666666666666...|\n",
      "|                          2|              [0.16666666666666...|\n",
      "|                          2|              [0.16666666666666...|\n",
      "|                          1|                             [0.0]|\n",
      "|                          1|                             [0.0]|\n",
      "|                          2|              [0.16666666666666...|\n",
      "|                          2|              [0.16666666666666...|\n",
      "|                          2|              [0.16666666666666...|\n",
      "|                          2|              [0.16666666666666...|\n",
      "|                          2|              [0.16666666666666...|\n",
      "|                          2|              [0.16666666666666...|\n",
      "|                          2|              [0.16666666666666...|\n",
      "|                          2|              [0.16666666666666...|\n",
      "|                          2|              [0.16666666666666...|\n",
      "|                          2|              [0.16666666666666...|\n",
      "|                          2|              [0.16666666666666...|\n",
      "|                          2|              [0.16666666666666...|\n",
      "|                          2|              [0.16666666666666...|\n",
      "+---------------------------+----------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import MinMaxScaler, VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"DataCleaning\").getOrCreate()\n",
    "\n",
    "# Read in the data  \n",
    "df = spark.read.csv('Road.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Use VectorAssembler to create a vector column\n",
    "assembler = VectorAssembler(inputCols=[\"Number_of_vehicles_involved\"], outputCol=\"Number_of_vehicles_involved_vec\")\n",
    "\n",
    "# Transform the DataFrame\n",
    "df_vector = assembler.transform(df)\n",
    "\n",
    "# Initialize Min-Max Scaler\n",
    "scaler = MinMaxScaler(inputCol=\"Number_of_vehicles_involved_vec\", outputCol=\"Number_of_vehicles_involved_scaled\")\n",
    "\n",
    "# Fit and Transform the data\n",
    "scalerModel = scaler.fit(df_vector)\n",
    "scaledData = scalerModel.transform(df_vector)\n",
    "\n",
    "# Show results\n",
    "scaledData.select(\"Number_of_vehicles_involved\", \"Number_of_vehicles_involved_scaled\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Number_of_casualties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------------+\n",
      "|Number_of_casualties|Number_of_casualties_scaled|\n",
      "+--------------------+---------------------------+\n",
      "|                   2|       [0.14285714285714...|\n",
      "|                   2|       [0.14285714285714...|\n",
      "|                   2|       [0.14285714285714...|\n",
      "|                   2|       [0.14285714285714...|\n",
      "|                   2|       [0.14285714285714...|\n",
      "|                   1|                      [0.0]|\n",
      "|                   1|                      [0.0]|\n",
      "|                   1|                      [0.0]|\n",
      "|                   1|                      [0.0]|\n",
      "|                   1|                      [0.0]|\n",
      "|                   1|                      [0.0]|\n",
      "|                   1|                      [0.0]|\n",
      "|                   2|       [0.14285714285714...|\n",
      "|                   2|       [0.14285714285714...|\n",
      "|                   2|       [0.14285714285714...|\n",
      "|                   3|       [0.2857142857142857]|\n",
      "|                   3|       [0.2857142857142857]|\n",
      "|                   3|       [0.2857142857142857]|\n",
      "|                   3|       [0.2857142857142857]|\n",
      "|                   1|                      [0.0]|\n",
      "+--------------------+---------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import MinMaxScaler, VectorAssembler\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"DataCleaning\").getOrCreate()\n",
    "\n",
    "# Read in the data  \n",
    "df = spark.read.csv('Road.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Use VectorAssembler to create a vector column\n",
    "assembler = VectorAssembler(inputCols=[\"Number_of_casualties\"], outputCol=\"Number_of_casualties_vec\")\n",
    "\n",
    "# Transform the DataFrame\n",
    "df_vector = assembler.transform(df)\n",
    "\n",
    "# Initialize Min-Max Scaler\n",
    "scaler = MinMaxScaler(inputCol=\"Number_of_casualties_vec\", outputCol=\"Number_of_casualties_scaled\")\n",
    "\n",
    "# Fit and Transform the data\n",
    "scalerModel = scaler.fit(df_vector)\n",
    "scaledData = scalerModel.transform(df_vector)\n",
    "\n",
    "# Show results\n",
    "scaledData.select(\"Number_of_casualties\", \"Number_of_casualties_scaled\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Vehicle_movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/09 02:32:46 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------------+------------------------+\n",
      "|Vehicle_movement|Vehicle_movement_Index|Vehicle_movement_Encoded|\n",
      "+----------------+----------------------+------------------------+\n",
      "|  Going straight|                   0.0|          (12,[0],[1.0])|\n",
      "|  Going straight|                   0.0|          (12,[0],[1.0])|\n",
      "|  Going straight|                   0.0|          (12,[0],[1.0])|\n",
      "|  Going straight|                   0.0|          (12,[0],[1.0])|\n",
      "|  Going straight|                   0.0|          (12,[0],[1.0])|\n",
      "|          U-Turn|                  10.0|         (12,[10],[1.0])|\n",
      "| Moving Backward|                   1.0|          (12,[1],[1.0])|\n",
      "|          U-Turn|                  10.0|         (12,[10],[1.0])|\n",
      "|  Going straight|                   0.0|          (12,[0],[1.0])|\n",
      "|          U-Turn|                  10.0|         (12,[10],[1.0])|\n",
      "|        Turnover|                   4.0|          (12,[4],[1.0])|\n",
      "|  Going straight|                   0.0|          (12,[0],[1.0])|\n",
      "|  Going straight|                   0.0|          (12,[0],[1.0])|\n",
      "|   Waiting to go|                  11.0|         (12,[11],[1.0])|\n",
      "|  Going straight|                   0.0|          (12,[0],[1.0])|\n",
      "|  Going straight|                   0.0|          (12,[0],[1.0])|\n",
      "|  Going straight|                   0.0|          (12,[0],[1.0])|\n",
      "|  Going straight|                   0.0|          (12,[0],[1.0])|\n",
      "| Moving Backward|                   1.0|          (12,[1],[1.0])|\n",
      "|  Going straight|                   0.0|          (12,[0],[1.0])|\n",
      "+----------------+----------------------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Read in the data  \n",
    "df = spark.read.csv('Road.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Filter out rows with null or 'Unknown' values in 'Vehicle_movement'\n",
    "df_filtered = df.filter((df.Vehicle_movement.isNotNull()) & (df.Vehicle_movement != 'Unknown'))\n",
    "\n",
    "# StringIndexer to convert the 'Vehicle_movement' column to numeric indices\n",
    "stringIndexer = StringIndexer(inputCol=\"Vehicle_movement\", outputCol=\"Vehicle_movement_Index\")\n",
    "\n",
    "# Fit the indexer to the data and transform it\n",
    "indexed = stringIndexer.fit(df).transform(df_filtered)\n",
    "\n",
    "# OneHotEncoder to convert indexed numbers to one-hot encoded values\n",
    "encoder = OneHotEncoder(inputCols=[\"Vehicle_movement_Index\"], outputCols=[\"Vehicle_movement_Encoded\"])\n",
    "\n",
    "# Fit and transform\n",
    "encoded = encoder.fit(indexed).transform(indexed)\n",
    "\n",
    "# Show the results\n",
    "encoded.select(\"Vehicle_movement\", \"Vehicle_movement_Index\", \"Vehicle_movement_Encoded\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Casualty_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+----------------------+\n",
      "| Casualty_class|Casualty_class_Index|Casualty_class_Encoded|\n",
      "+---------------+--------------------+----------------------+\n",
      "|Driver or rider|                 0.0|         (3,[0],[1.0])|\n",
      "|     Pedestrian|                 2.0|         (3,[2],[1.0])|\n",
      "|Driver or rider|                 0.0|         (3,[0],[1.0])|\n",
      "|Driver or rider|                 0.0|         (3,[0],[1.0])|\n",
      "|     Pedestrian|                 2.0|         (3,[2],[1.0])|\n",
      "|      Passenger|                 3.0|             (3,[],[])|\n",
      "|Driver or rider|                 0.0|         (3,[0],[1.0])|\n",
      "|Driver or rider|                 0.0|         (3,[0],[1.0])|\n",
      "|Driver or rider|                 0.0|         (3,[0],[1.0])|\n",
      "|Driver or rider|                 0.0|         (3,[0],[1.0])|\n",
      "|      Passenger|                 3.0|             (3,[],[])|\n",
      "|      Passenger|                 3.0|             (3,[],[])|\n",
      "|Driver or rider|                 0.0|         (3,[0],[1.0])|\n",
      "|     Pedestrian|                 2.0|         (3,[2],[1.0])|\n",
      "|Driver or rider|                 0.0|         (3,[0],[1.0])|\n",
      "|Driver or rider|                 0.0|         (3,[0],[1.0])|\n",
      "|Driver or rider|                 0.0|         (3,[0],[1.0])|\n",
      "|      Passenger|                 3.0|             (3,[],[])|\n",
      "|Driver or rider|                 0.0|         (3,[0],[1.0])|\n",
      "|     Pedestrian|                 2.0|         (3,[2],[1.0])|\n",
      "+---------------+--------------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Read in the data  \n",
    "df = spark.read.csv('Road.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Filter out rows with null or 'Unknown' values in 'Casualty_class'\n",
    "df_filtered = df.filter((df.Casualty_class.isNotNull()) & (df.Casualty_class != 'na'))\n",
    "\n",
    "# StringIndexer to convert the 'Casualty_class' column to numeric indices\n",
    "stringIndexer = StringIndexer(inputCol=\"Casualty_class\", outputCol=\"Casualty_class_Index\")\n",
    "\n",
    "# Fit the indexer to the data and transform it\n",
    "indexed = stringIndexer.fit(df).transform(df_filtered)\n",
    "\n",
    "# OneHotEncoder to convert indexed numbers to one-hot encoded values\n",
    "encoder = OneHotEncoder(inputCols=[\"Casualty_class_Index\"], outputCols=[\"Casualty_class_Encoded\"])\n",
    "\n",
    "# Fit and transform\n",
    "encoded = encoder.fit(indexed).transform(indexed)\n",
    "\n",
    "# Show the results\n",
    "encoded.select(\"Casualty_class\", \"Casualty_class_Index\", \"Casualty_class_Encoded\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Sex_of_casualty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------------+-----------------------+\n",
      "|Sex_of_casualty|Sex_of_casualty_Index|Sex_of_casualty_Encoded|\n",
      "+---------------+---------------------+-----------------------+\n",
      "|           Male|                  0.0|          (2,[0],[1.0])|\n",
      "|         Female|                  2.0|              (2,[],[])|\n",
      "|           Male|                  0.0|          (2,[0],[1.0])|\n",
      "|         Female|                  2.0|              (2,[],[])|\n",
      "|           Male|                  0.0|          (2,[0],[1.0])|\n",
      "|           Male|                  0.0|          (2,[0],[1.0])|\n",
      "|           Male|                  0.0|          (2,[0],[1.0])|\n",
      "|         Female|                  2.0|              (2,[],[])|\n",
      "|           Male|                  0.0|          (2,[0],[1.0])|\n",
      "|           Male|                  0.0|          (2,[0],[1.0])|\n",
      "|         Female|                  2.0|              (2,[],[])|\n",
      "|           Male|                  0.0|          (2,[0],[1.0])|\n",
      "|         Female|                  2.0|              (2,[],[])|\n",
      "|           Male|                  0.0|          (2,[0],[1.0])|\n",
      "|           Male|                  0.0|          (2,[0],[1.0])|\n",
      "|           Male|                  0.0|          (2,[0],[1.0])|\n",
      "|           Male|                  0.0|          (2,[0],[1.0])|\n",
      "|         Female|                  2.0|              (2,[],[])|\n",
      "|         Female|                  2.0|              (2,[],[])|\n",
      "|           Male|                  0.0|          (2,[0],[1.0])|\n",
      "+---------------+---------------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Read in the data  \n",
    "df = spark.read.csv('Road.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Filter out rows with null or 'Unknown' values in 'Sex_of_casualty'\n",
    "df_filtered = df.filter((df.Sex_of_casualty.isNotNull()) & (df.Sex_of_casualty != 'na'))\n",
    "\n",
    "# StringIndexer to convert the 'Sex_of_casualty' column to numeric indices\n",
    "stringIndexer = StringIndexer(inputCol=\"Sex_of_casualty\", outputCol=\"Sex_of_casualty_Index\")\n",
    "\n",
    "# Fit the indexer to the data and transform it\n",
    "indexed = stringIndexer.fit(df).transform(df_filtered)\n",
    "\n",
    "# OneHotEncoder to convert indexed numbers to one-hot encoded values\n",
    "encoder = OneHotEncoder(inputCols=[\"Sex_of_casualty_Index\"], outputCols=[\"Sex_of_casualty_Encoded\"])\n",
    "\n",
    "# Fit and transform\n",
    "encoded = encoder.fit(indexed).transform(indexed)\n",
    "\n",
    "# Show the results\n",
    "encoded.select(\"Sex_of_casualty\", \"Sex_of_casualty_Index\", \"Sex_of_casualty_Encoded\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Age_band_of_casualty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------------+----------------------------+\n",
      "|Age_band_of_casualty|Age_band_of_casualty_Index|Age_band_of_casualty_Encoded|\n",
      "+--------------------+--------------------------+----------------------------+\n",
      "|               31-50|                       2.0|               (5,[2],[1.0])|\n",
      "|               18-30|                       1.0|               (5,[1],[1.0])|\n",
      "|               31-50|                       2.0|               (5,[2],[1.0])|\n",
      "|               18-30|                       1.0|               (5,[1],[1.0])|\n",
      "|            Under 18|                       3.0|               (5,[3],[1.0])|\n",
      "|               18-30|                       1.0|               (5,[1],[1.0])|\n",
      "|               18-30|                       1.0|               (5,[1],[1.0])|\n",
      "|               18-30|                       1.0|               (5,[1],[1.0])|\n",
      "|            Under 18|                       3.0|               (5,[3],[1.0])|\n",
      "|               18-30|                       1.0|               (5,[1],[1.0])|\n",
      "|               31-50|                       2.0|               (5,[2],[1.0])|\n",
      "|               18-30|                       1.0|               (5,[1],[1.0])|\n",
      "|               31-50|                       2.0|               (5,[2],[1.0])|\n",
      "|            Under 18|                       3.0|               (5,[3],[1.0])|\n",
      "|               31-50|                       2.0|               (5,[2],[1.0])|\n",
      "|               18-30|                       1.0|               (5,[1],[1.0])|\n",
      "|               18-30|                       1.0|               (5,[1],[1.0])|\n",
      "|               18-30|                       1.0|               (5,[1],[1.0])|\n",
      "|               18-30|                       1.0|               (5,[1],[1.0])|\n",
      "|            Under 18|                       3.0|               (5,[3],[1.0])|\n",
      "+--------------------+--------------------------+----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Read in the data  \n",
    "df = spark.read.csv('Road.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Filter out rows with null or 'Unknown' values in 'Age_band_of_casualty'\n",
    "df_filtered = df.filter((df.Age_band_of_casualty.isNotNull()) & (df.Age_band_of_casualty != 'na'))\n",
    "\n",
    "# StringIndexer to convert the 'Age_band_of_casualty' column to numeric indices\n",
    "stringIndexer = StringIndexer(inputCol=\"Age_band_of_casualty\", outputCol=\"Age_band_of_casualty_Index\")\n",
    "\n",
    "# Fit the indexer to the data and transform it\n",
    "indexed = stringIndexer.fit(df).transform(df_filtered)\n",
    "\n",
    "# OneHotEncoder to convert indexed numbers to one-hot encoded values\n",
    "encoder = OneHotEncoder(inputCols=[\"Age_band_of_casualty_Index\"], outputCols=[\"Age_band_of_casualty_Encoded\"])\n",
    "\n",
    "# Fit and transform\n",
    "encoded = encoder.fit(indexed).transform(indexed)\n",
    "\n",
    "# Show the results\n",
    "encoded.select(\"Age_band_of_casualty\", \"Age_band_of_casualty_Index\", \"Age_band_of_casualty_Encoded\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Casualty_severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------------+-------------------------+\n",
      "|Casualty_severity|Casualty_severity_Index|Casualty_severity_Encoded|\n",
      "+-----------------+-----------------------+-------------------------+\n",
      "|                3|                    0.0|            (3,[0],[1.0])|\n",
      "|                3|                    0.0|            (3,[0],[1.0])|\n",
      "|                3|                    0.0|            (3,[0],[1.0])|\n",
      "|                3|                    0.0|            (3,[0],[1.0])|\n",
      "|                3|                    0.0|            (3,[0],[1.0])|\n",
      "|                3|                    0.0|            (3,[0],[1.0])|\n",
      "|                3|                    0.0|            (3,[0],[1.0])|\n",
      "|                3|                    0.0|            (3,[0],[1.0])|\n",
      "|                3|                    0.0|            (3,[0],[1.0])|\n",
      "|                3|                    0.0|            (3,[0],[1.0])|\n",
      "|                3|                    0.0|            (3,[0],[1.0])|\n",
      "|                3|                    0.0|            (3,[0],[1.0])|\n",
      "|                3|                    0.0|            (3,[0],[1.0])|\n",
      "|                3|                    0.0|            (3,[0],[1.0])|\n",
      "|                3|                    0.0|            (3,[0],[1.0])|\n",
      "|                2|                    2.0|            (3,[2],[1.0])|\n",
      "|                3|                    0.0|            (3,[0],[1.0])|\n",
      "|                3|                    0.0|            (3,[0],[1.0])|\n",
      "|                3|                    0.0|            (3,[0],[1.0])|\n",
      "|                3|                    0.0|            (3,[0],[1.0])|\n",
      "+-----------------+-----------------------+-------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Read in the data  \n",
    "df = spark.read.csv('Road.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Filter out rows with null or 'Unknown' values in 'Casualty_severity'\n",
    "df_filtered = df.filter((df.Casualty_severity.isNotNull()) & (df.Casualty_severity != 'na'))\n",
    "\n",
    "# StringIndexer to convert the 'Casualty_severity' column to numeric indices\n",
    "stringIndexer = StringIndexer(inputCol=\"Casualty_severity\", outputCol=\"Casualty_severity_Index\")\n",
    "\n",
    "# Fit the indexer to the data and transform it\n",
    "indexed = stringIndexer.fit(df).transform(df_filtered)\n",
    "\n",
    "# OneHotEncoder to convert indexed numbers to one-hot encoded values\n",
    "encoder = OneHotEncoder(inputCols=[\"Casualty_severity_Index\"], outputCols=[\"Casualty_severity_Encoded\"])\n",
    "\n",
    "# Fit and transform\n",
    "encoded = encoder.fit(indexed).transform(indexed)\n",
    "\n",
    "# Show the results\n",
    "encoded.select(\"Casualty_severity\", \"Casualty_severity_Index\", \"Casualty_severity_Encoded\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Work_of_casuality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------------+-------------------------+\n",
      "|Work_of_casuality|Work_of_casuality_Index|Work_of_casuality_Encoded|\n",
      "+-----------------+-----------------------+-------------------------+\n",
      "|           Driver|                    0.0|            (6,[0],[1.0])|\n",
      "|           Driver|                    0.0|            (6,[0],[1.0])|\n",
      "|           Driver|                    0.0|            (6,[0],[1.0])|\n",
      "|           Driver|                    0.0|            (6,[0],[1.0])|\n",
      "|           Driver|                    0.0|            (6,[0],[1.0])|\n",
      "|           Driver|                    0.0|            (6,[0],[1.0])|\n",
      "|           Driver|                    0.0|            (6,[0],[1.0])|\n",
      "|           Driver|                    0.0|            (6,[0],[1.0])|\n",
      "|            Other|                    3.0|            (6,[3],[1.0])|\n",
      "|           Driver|                    0.0|            (6,[0],[1.0])|\n",
      "|           Driver|                    0.0|            (6,[0],[1.0])|\n",
      "|           Driver|                    0.0|            (6,[0],[1.0])|\n",
      "|       Unemployed|                    5.0|            (6,[5],[1.0])|\n",
      "|           Driver|                    0.0|            (6,[0],[1.0])|\n",
      "|           Driver|                    0.0|            (6,[0],[1.0])|\n",
      "|           Driver|                    0.0|            (6,[0],[1.0])|\n",
      "|           Driver|                    0.0|            (6,[0],[1.0])|\n",
      "|         Employee|                    2.0|            (6,[2],[1.0])|\n",
      "|           Driver|                    0.0|            (6,[0],[1.0])|\n",
      "|    Self-employed|                    1.0|            (6,[1],[1.0])|\n",
      "+-----------------+-----------------------+-------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Read in the data  \n",
    "df = spark.read.csv('Road.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Filter out rows with null or 'Unknown' values in 'Work_of_casuality'\n",
    "df_filtered = df.filter((df.Work_of_casuality.isNotNull()) & (df.Work_of_casuality != 'Unknown'))\n",
    "\n",
    "# StringIndexer to convert the 'Work_of_casuality' column to numeric indices\n",
    "stringIndexer = StringIndexer(inputCol=\"Work_of_casuality\", outputCol=\"Work_of_casuality_Index\")\n",
    "\n",
    "# Fit the indexer to the data and transform it\n",
    "indexed = stringIndexer.fit(df).transform(df_filtered)\n",
    "\n",
    "# OneHotEncoder to convert indexed numbers to one-hot encoded values\n",
    "encoder = OneHotEncoder(inputCols=[\"Work_of_casuality_Index\"], outputCols=[\"Work_of_casuality_Encoded\"])\n",
    "\n",
    "# Fit and transform\n",
    "encoded = encoder.fit(indexed).transform(indexed)\n",
    "\n",
    "# Show the results\n",
    "encoded.select(\"Work_of_casuality\", \"Work_of_casuality_Index\", \"Work_of_casuality_Encoded\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Fitness_of_casuality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------------+----------------------------+\n",
      "|Fitness_of_casuality|Fitness_of_casuality_Index|Fitness_of_casuality_Encoded|\n",
      "+--------------------+--------------------------+----------------------------+\n",
      "|              Normal|                       0.0|               (3,[0],[1.0])|\n",
      "|              Normal|                       0.0|               (3,[0],[1.0])|\n",
      "|              Normal|                       0.0|               (3,[0],[1.0])|\n",
      "|              Normal|                       0.0|               (3,[0],[1.0])|\n",
      "|              Normal|                       0.0|               (3,[0],[1.0])|\n",
      "|              Normal|                       0.0|               (3,[0],[1.0])|\n",
      "|              Normal|                       0.0|               (3,[0],[1.0])|\n",
      "|              Normal|                       0.0|               (3,[0],[1.0])|\n",
      "|              Normal|                       0.0|               (3,[0],[1.0])|\n",
      "|              Normal|                       0.0|               (3,[0],[1.0])|\n",
      "|              Normal|                       0.0|               (3,[0],[1.0])|\n",
      "|              Normal|                       0.0|               (3,[0],[1.0])|\n",
      "|              Normal|                       0.0|               (3,[0],[1.0])|\n",
      "|              Normal|                       0.0|               (3,[0],[1.0])|\n",
      "|              Normal|                       0.0|               (3,[0],[1.0])|\n",
      "|              Normal|                       0.0|               (3,[0],[1.0])|\n",
      "|              Normal|                       0.0|               (3,[0],[1.0])|\n",
      "|              Normal|                       0.0|               (3,[0],[1.0])|\n",
      "|              Normal|                       0.0|               (3,[0],[1.0])|\n",
      "|              Normal|                       0.0|               (3,[0],[1.0])|\n",
      "+--------------------+--------------------------+----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Read in the data  \n",
    "df = spark.read.csv('Road.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Filter out rows with null or 'Unknown' values in 'Fitness_of_casuality'\n",
    "df_filtered = df.filter((df.Fitness_of_casuality.isNotNull()) & (df.Fitness_of_casuality != 'Unknown'))\n",
    "\n",
    "# Replace 'NormalNormal' with 'Normal' in 'Fitness_of_casuality'\n",
    "df_filtered = df_filtered.withColumn(\"Fitness_of_casuality\", \n",
    "                   when(df.Fitness_of_casuality == \"NormalNormal\", \"Normal\")\n",
    "                   .otherwise(df.Fitness_of_casuality))\n",
    "\n",
    "# StringIndexer to convert the 'Fitness_of_casuality' column to numeric indices\n",
    "stringIndexer = StringIndexer(inputCol=\"Fitness_of_casuality\", outputCol=\"Fitness_of_casuality_Index\")\n",
    "\n",
    "# Fit the indexer to the data and transform it\n",
    "indexed = stringIndexer.fit(df_filtered).transform(df_filtered)\n",
    "\n",
    "# OneHotEncoder to convert indexed numbers to one-hot encoded values\n",
    "# encoder = OneHotEncoder(inputCols=[\"Fitness_of_casuality_Index\"], outputCols=[\"Fitness_of_casuality_Encoded\"], dropLast=False)\n",
    "encoder = OneHotEncoder(inputCols=[\"Fitness_of_casuality_Index\"], outputCols=[\"Fitness_of_casuality_Encoded\"])\n",
    "\n",
    "# Fit and transform\n",
    "encoded = encoder.fit(indexed).transform(indexed)\n",
    "\n",
    "# Show the results\n",
    "encoded.select(\"Fitness_of_casuality\", \"Fitness_of_casuality_Index\", \"Fitness_of_casuality_Encoded\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Pedestrian_movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------------+---------------------------+\n",
      "| Pedestrian_movement|Pedestrian_movement_Index|Pedestrian_movement_Encoded|\n",
      "+--------------------+-------------------------+---------------------------+\n",
      "|    Not a Pedestrian|                      0.0|              (7,[0],[1.0])|\n",
      "|    Not a Pedestrian|                      0.0|              (7,[0],[1.0])|\n",
      "|    Not a Pedestrian|                      0.0|              (7,[0],[1.0])|\n",
      "|    Not a Pedestrian|                      0.0|              (7,[0],[1.0])|\n",
      "|    Not a Pedestrian|                      0.0|              (7,[0],[1.0])|\n",
      "|    Not a Pedestrian|                      0.0|              (7,[0],[1.0])|\n",
      "|    Not a Pedestrian|                      0.0|              (7,[0],[1.0])|\n",
      "|    Not a Pedestrian|                      0.0|              (7,[0],[1.0])|\n",
      "|Crossing from dri...|                      2.0|              (7,[2],[1.0])|\n",
      "|    Not a Pedestrian|                      0.0|              (7,[0],[1.0])|\n",
      "|    Not a Pedestrian|                      0.0|              (7,[0],[1.0])|\n",
      "|    Not a Pedestrian|                      0.0|              (7,[0],[1.0])|\n",
      "|    Not a Pedestrian|                      0.0|              (7,[0],[1.0])|\n",
      "|    Not a Pedestrian|                      0.0|              (7,[0],[1.0])|\n",
      "|    Not a Pedestrian|                      0.0|              (7,[0],[1.0])|\n",
      "|    Not a Pedestrian|                      0.0|              (7,[0],[1.0])|\n",
      "|    Not a Pedestrian|                      0.0|              (7,[0],[1.0])|\n",
      "|    Not a Pedestrian|                      0.0|              (7,[0],[1.0])|\n",
      "|    Not a Pedestrian|                      0.0|              (7,[0],[1.0])|\n",
      "|    Not a Pedestrian|                      0.0|              (7,[0],[1.0])|\n",
      "+--------------------+-------------------------+---------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Read in the data  \n",
    "df = spark.read.csv('Road.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Filter out rows with null or 'Unknown' values in 'Pedestrian_movement'\n",
    "df_filtered = df.filter((df.Pedestrian_movement.isNotNull()) & (df.Pedestrian_movement != 'Unknown or other'))\n",
    "\n",
    "# StringIndexer to convert the 'Pedestrian_movement' column to numeric indices\n",
    "stringIndexer = StringIndexer(inputCol=\"Pedestrian_movement\", outputCol=\"Pedestrian_movement_Index\")\n",
    "\n",
    "# Fit the indexer to the data and transform it\n",
    "indexed = stringIndexer.fit(df_filtered).transform(df_filtered)\n",
    "\n",
    "# OneHotEncoder to convert indexed numbers to one-hot encoded values\n",
    "encoder = OneHotEncoder(inputCols=[\"Pedestrian_movement_Index\"], outputCols=[\"Pedestrian_movement_Encoded\"])\n",
    "\n",
    "# Fit and transform\n",
    "encoded = encoder.fit(indexed).transform(indexed)\n",
    "\n",
    "# Show the results\n",
    "encoded.select(\"Pedestrian_movement\", \"Pedestrian_movement_Index\", \"Pedestrian_movement_Encoded\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Cause_of_accident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------------+-------------------------+\n",
      "|   Cause_of_accident|Cause_of_accident_Index|Cause_of_accident_Encoded|\n",
      "+--------------------+-----------------------+-------------------------+\n",
      "|     Moving Backward|                    5.0|           (17,[5],[1.0])|\n",
      "|          Overtaking|                    7.0|           (17,[7],[1.0])|\n",
      "|Changing lane to ...|                    2.0|           (17,[2],[1.0])|\n",
      "|Changing lane to ...|                    1.0|           (17,[1],[1.0])|\n",
      "|          Overtaking|                    7.0|           (17,[7],[1.0])|\n",
      "|         Overloading|                   15.0|          (17,[15],[1.0])|\n",
      "|No priority to ve...|                    4.0|           (17,[4],[1.0])|\n",
      "|Changing lane to ...|                    1.0|           (17,[1],[1.0])|\n",
      "|     Moving Backward|                    5.0|           (17,[5],[1.0])|\n",
      "|Changing lane to ...|                    2.0|           (17,[2],[1.0])|\n",
      "|No priority to pe...|                    6.0|           (17,[6],[1.0])|\n",
      "|       No distancing|                    0.0|           (17,[0],[1.0])|\n",
      "|No priority to ve...|                    4.0|           (17,[4],[1.0])|\n",
      "|       No distancing|                    0.0|           (17,[0],[1.0])|\n",
      "|       No distancing|                    0.0|           (17,[0],[1.0])|\n",
      "|     Moving Backward|                    5.0|           (17,[5],[1.0])|\n",
      "|Changing lane to ...|                    1.0|           (17,[1],[1.0])|\n",
      "|No priority to pe...|                    6.0|           (17,[6],[1.0])|\n",
      "|     Moving Backward|                    5.0|           (17,[5],[1.0])|\n",
      "|Getting off the v...|                   10.0|          (17,[10],[1.0])|\n",
      "+--------------------+-----------------------+-------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Read in the data  \n",
    "df = spark.read.csv('Road.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Filter out rows with null or 'Unknown' values in 'Cause_of_accident'\n",
    "df_filtered = df.filter((df.Cause_of_accident.isNotNull()) & (df.Cause_of_accident != 'Unknown') & (df.Cause_of_accident != 'Other'))\n",
    "\n",
    "# StringIndexer to convert the 'Cause_of_accident' column to numeric indices\n",
    "stringIndexer = StringIndexer(inputCol=\"Cause_of_accident\", outputCol=\"Cause_of_accident_Index\")\n",
    "\n",
    "# Fit the indexer to the data and transform it\n",
    "indexed = stringIndexer.fit(df_filtered).transform(df_filtered)\n",
    "\n",
    "# OneHotEncoder to convert indexed numbers to one-hot encoded values\n",
    "encoder = OneHotEncoder(inputCols=[\"Cause_of_accident_Index\"], outputCols=[\"Cause_of_accident_Encoded\"])\n",
    "\n",
    "# Fit and transform\n",
    "encoded = encoder.fit(indexed).transform(indexed)\n",
    "\n",
    "# Show the results\n",
    "encoded.select(\"Cause_of_accident\", \"Cause_of_accident_Index\", \"Cause_of_accident_Encoded\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Accident_severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------------+-------------------------+\n",
      "|Accident_severity|Accident_severity_Index|Accident_severity_Encoded|\n",
      "+-----------------+-----------------------+-------------------------+\n",
      "|    Slight Injury|                    0.0|            (2,[0],[1.0])|\n",
      "|    Slight Injury|                    0.0|            (2,[0],[1.0])|\n",
      "|   Serious Injury|                    1.0|            (2,[1],[1.0])|\n",
      "|    Slight Injury|                    0.0|            (2,[0],[1.0])|\n",
      "|    Slight Injury|                    0.0|            (2,[0],[1.0])|\n",
      "|    Slight Injury|                    0.0|            (2,[0],[1.0])|\n",
      "|    Slight Injury|                    0.0|            (2,[0],[1.0])|\n",
      "|    Slight Injury|                    0.0|            (2,[0],[1.0])|\n",
      "|    Slight Injury|                    0.0|            (2,[0],[1.0])|\n",
      "|   Serious Injury|                    1.0|            (2,[1],[1.0])|\n",
      "|   Serious Injury|                    1.0|            (2,[1],[1.0])|\n",
      "|   Serious Injury|                    1.0|            (2,[1],[1.0])|\n",
      "|    Slight Injury|                    0.0|            (2,[0],[1.0])|\n",
      "|    Slight Injury|                    0.0|            (2,[0],[1.0])|\n",
      "|   Serious Injury|                    1.0|            (2,[1],[1.0])|\n",
      "|   Serious Injury|                    1.0|            (2,[1],[1.0])|\n",
      "|    Slight Injury|                    0.0|            (2,[0],[1.0])|\n",
      "|   Serious Injury|                    1.0|            (2,[1],[1.0])|\n",
      "|    Slight Injury|                    0.0|            (2,[0],[1.0])|\n",
      "|   Serious Injury|                    1.0|            (2,[1],[1.0])|\n",
      "+-----------------+-----------------------+-------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Read in the data  \n",
    "df = spark.read.csv('Road.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Filter out rows with null or 'Unknown' values in 'Accident_severity'\n",
    "df_filtered = df.filter((df.Accident_severity.isNotNull()) & (df.Accident_severity != 'Unknown'))\n",
    "\n",
    "# StringIndexer to convert the 'Accident_severity' column to numeric indices\n",
    "stringIndexer = StringIndexer(inputCol=\"Accident_severity\", outputCol=\"Accident_severity_Index\")\n",
    "\n",
    "# Fit the indexer to the data and transform it\n",
    "indexed = stringIndexer.fit(df_filtered).transform(df_filtered)\n",
    "\n",
    "# OneHotEncoder to convert indexed numbers to one-hot encoded values\n",
    "encoder = OneHotEncoder(inputCols=[\"Accident_severity_Index\"], outputCols=[\"Accident_severity_Encoded\"])\n",
    "\n",
    "# Fit and transform\n",
    "encoded = encoder.fit(indexed).transform(indexed)\n",
    "\n",
    "# Show the results\n",
    "encoded.select(\"Accident_severity\", \"Accident_severity_Index\", \"Accident_severity_Encoded\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "# Add an index column to the original DataFrame\n",
    "df = df.withColumn(\"index\", monotonically_increasing_id())\n",
    "\n",
    "# List of columns to be encoded\n",
    "columns_to_encode = [\n",
    "    #\"Age_band_of_driver\", \"Sex_of_driver\", \"Educational_level\", \n",
    "    #\"Vehicle_driver_relation\", \"Driving_experience\", \"Type_of_vehicle\", \n",
    "    \"Owner_of_vehicle\", \"Service_year_of_vehicle\", \"Defect_of_vehicle\", \n",
    "    #\"Area_accident_occured\", \"Lanes_or_Medians\", \"Road_allignment\", \n",
    "    #\"Types_of_Junction\", \"Road_surface_type\", \"Road_surface_conditions\", \n",
    "    #\"Light_conditions\"\n",
    "]\n",
    "\n",
    "for col in columns_to_encode:\n",
    "    # Filter out rows with null or 'Unknown' values in the current column\n",
    "    df_filtered = df.filter((df[col].isNotNull()) & (df[col] != 'Unknown'))\n",
    "\n",
    "    # StringIndexer to convert the current column to numeric indices, handling unseen labels\n",
    "    stringIndexer = StringIndexer(inputCol=col, outputCol=f\"{col}_Index\", handleInvalid=\"keep\")\n",
    "\n",
    "    # Fit the indexer to the data and transform it\n",
    "    indexed = stringIndexer.fit(df_filtered).transform(df_filtered)\n",
    "\n",
    "    # OneHotEncoder to convert indexed numbers to one-hot encoded values\n",
    "    encoder = OneHotEncoder(inputCols=[f\"{col}_Index\"], outputCols=[f\"{col}_Encoded\"])\n",
    "\n",
    "    # Check if the encoded column already exists\n",
    "    if f\"{col}_Encoded\" in indexed.columns:\n",
    "        indexed = indexed.drop(f\"{col}_Encoded\")\n",
    "\n",
    "    # Fit and transform\n",
    "    encoded = encoder.fit(indexed).transform(indexed)\n",
    "\n",
    "    # Add the same index column to the encoded DataFrame\n",
    "    encoded = encoded.withColumn(\"index\", monotonically_increasing_id())\n",
    "\n",
    "    # Join using the index\n",
    "    df = df.join(encoded.select(\"index\", f\"{col}_Encoded\"), on=[\"index\"])\n",
    "\n",
    "# Optionally, you can drop the index column after joining if it's no longer needed\n",
    "df = df.drop(\"index\")\n",
    "\n",
    "# Show the results\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

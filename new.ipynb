{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pinpoint specific geographic locations and temporal patterns where accidents occur most frequently, enabling targeted measures to enhance road safety."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/18 14:02:53 WARN Utils: Your hostname, Kais-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.13.110.78 instead (on interface en0)\n",
      "23/12/18 14:02:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/18 14:02:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Accident Hotspots Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.csv('Road.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Time',\n",
       " 'Day_of_week',\n",
       " 'Age_band_of_driver',\n",
       " 'Sex_of_driver',\n",
       " 'Educational_level',\n",
       " 'Vehicle_driver_relation',\n",
       " 'Driving_experience',\n",
       " 'Type_of_vehicle',\n",
       " 'Owner_of_vehicle',\n",
       " 'Service_year_of_vehicle',\n",
       " 'Defect_of_vehicle',\n",
       " 'Area_accident_occured',\n",
       " 'Lanes_or_Medians',\n",
       " 'Road_allignment',\n",
       " 'Types_of_Junction',\n",
       " 'Road_surface_type',\n",
       " 'Road_surface_conditions',\n",
       " 'Light_conditions',\n",
       " 'Weather_conditions',\n",
       " 'Type_of_collision',\n",
       " 'Number_of_vehicles_involved',\n",
       " 'Number_of_casualties',\n",
       " 'Vehicle_movement',\n",
       " 'Casualty_class',\n",
       " 'Sex_of_casualty',\n",
       " 'Age_band_of_casualty',\n",
       " 'Casualty_severity',\n",
       " 'Work_of_casuality',\n",
       " 'Fitness_of_casuality',\n",
       " 'Pedestrian_movement',\n",
       " 'Cause_of_accident',\n",
       " 'Accident_severity']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12316"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Analysis - Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sex_of_driver vs Accident_severity (Not statistically Significant)\n",
    "If a correlation is found, it might suggest that certain driver demographics are more prone to be involved in severe accidents in specific areas. This can lead to targeted safety programs or interventions focused on these demographics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12138"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Assuming 'df' is your PySpark DataFrame and 'Sex_of_driver' is the column of interest\n",
    "df_filter = df.filter(df['Sex_of_driver'].isNotNull() & (df['Sex_of_driver'] != 'unknown') & (df['Sex_of_driver'] != 'Unknown'))\n",
    "df_filter = df_filter.filter(df_filter['Accident_severity'].isNotNull() & (df_filter['Accident_severity'] != 'unknown') & (df_filter['Accident_severity'] != 'Unknown'))\n",
    "\n",
    "df_filter.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from scipy.stats import chi2_contingency\n",
    "import pandas as pd\n",
    "\n",
    "def perform_chi_square_test(df, column1, column2):\n",
    "    \"\"\"\n",
    "    Perform a Chi-Square test on two categorical columns of a PySpark DataFrame.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): PySpark DataFrame.\n",
    "    column1 (str): Name of the first column.\n",
    "    column2 (str): Name of the second column.\n",
    "\n",
    "    Returns:\n",
    "    Tuple: Chi-square test statistic, p-value, degrees of freedom, and expected frequencies.\n",
    "    \"\"\"\n",
    "    # Create the contingency table\n",
    "    contingency_table = df.groupBy(column1, column2).count()\n",
    "\n",
    "    # Convert to Pandas DataFrame for chi-square test\n",
    "    contingency_pd = contingency_table.toPandas()\n",
    "    contingency_pd_pivot = contingency_pd.pivot(index=column1, columns=column2, values='count').fillna(0)\n",
    "\n",
    "    # Perform the Chi-Square Test\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency_pd_pivot)\n",
    "    significance_level = 0.05\n",
    "\n",
    "\n",
    "    # Return the results\n",
    "    if p <= significance_level:\n",
    "        print(column2, ' - Dependent (reject H0)')\n",
    "    else:\n",
    "        print(column2, '- Independent (H0 holds true)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"ChiSquareTest\").getOrCreate()\n",
    "\n",
    "# Assuming df is your PySpark DataFrame\n",
    "# To use the function, call it with the DataFrame and column names\n",
    "chi2, p, dof, expected = perform_chi_square_test(df, 'Sex_of_driver', 'Accident_severity')\n",
    "print(chi2, p, dof, expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Independent (H0 holds true)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from scipy.stats import chi2_contingency\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"ChiSquareTest\").getOrCreate()\n",
    "\n",
    "# Assuming df is your PySpark DataFrame\n",
    "# Create the contingency table\n",
    "contingency_table = df.groupBy('Sex_of_driver', 'Accident_severity').count()\n",
    "\n",
    "# Convert to Pandas DataFrame for chi-square test\n",
    "contingency_pd = contingency_table.toPandas()\n",
    "contingency_pd_pivot = contingency_pd.pivot(index='Sex_of_driver', columns='Accident_severity', values='count').fillna(0)\n",
    "\n",
    "# Perform the Chi-Square Test\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_pd_pivot)\n",
    "\n",
    "# Compare the P-Value to the significance level\n",
    "# If P-Value is less than or equal to the significance level, then the variables are dependent\n",
    "# If P-Value is greater than the significance level, then the variables are independent\n",
    "significance_level = 0.05\n",
    "if p <= significance_level:\n",
    "    print('Dependent (reject H0)')\n",
    "else:\n",
    "    print('Independent (H0 holds true)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Educational_level vs Accident_severity (>P, Weak Association)\n",
    "Correlation here could indicate the need for educational campaigns or driver safety programs tailored to different education levels, potentially reducing accident severity in specific regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Assuming 'df' is your PySpark DataFrame and 'Sex_of_driver' is the column of interest\n",
    "df_filter = df.filter(df['Educational_level'].isNotNull() & (df['Educational_level'] != 'unknown') & (df['Educational_level'] != 'Unknown'))\n",
    "df_filter = df_filter.filter(df_filter['Accident_severity'].isNotNull() & (df_filter['Accident_severity'] != 'unknown') & (df_filter['Accident_severity'] != 'Unknown'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from scipy.stats import chi2_contingency\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"ChiSquareTest\").getOrCreate()\n",
    "\n",
    "# Assuming df is your PySpark DataFrame\n",
    "# Create the contingency table\n",
    "contingency_table = df.groupBy('Educational_level', 'Accident_severity').count()\n",
    "\n",
    "# Convert to Pandas DataFrame for chi-square test\n",
    "contingency_pd = contingency_table.toPandas()\n",
    "contingency_pd_pivot = contingency_pd.pivot(index='Educational_level', columns='Accident_severity', values='count').fillna(0)\n",
    "\n",
    "# Perform the Chi-Square Test\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_pd_pivot)\n",
    "\n",
    "# Output the results\n",
    "print(chi2, p, dof, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day_of_week vs Type_of_collision (<P, but not related to accident frequency or location)\n",
    "Identifying days with higher frequencies of certain types of collisions can inform scheduling for increased road safety measures or traffic enforcement on those specific days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Assuming 'df' is your PySpark DataFrame and 'Sex_of_driver' is the column of interest\n",
    "df_filter = df.filter(df['Day_of_week'].isNotNull() & (df['Day_of_week'] != 'unknown') & (df['Day_of_week'] != 'Unknown'))\n",
    "df_filter = df_filter.filter(df_filter['Type_of_collision'].isNotNull() & (df_filter['Type_of_collision'] != 'Other'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from scipy.stats import chi2_contingency\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"ChiSquareTest\").getOrCreate()\n",
    "\n",
    "# Assuming df is your PySpark DataFrame\n",
    "# Create the contingency table\n",
    "contingency_table = df.groupBy('Day_of_week', 'Type_of_collision').count()\n",
    "\n",
    "# Convert to Pandas DataFrame for chi-square test\n",
    "contingency_pd = contingency_table.toPandas()\n",
    "contingency_pd_pivot = contingency_pd.pivot(index='Day_of_week', columns='Type_of_collision', values='count').fillna(0)\n",
    "\n",
    "# Perform the Chi-Square Test\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_pd_pivot)\n",
    "\n",
    "# Output the results\n",
    "print(chi2, p, dof, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Weather_conditions vs Accident_severity*\n",
    "This correlation can help in predicting accident severity based on weather conditions. It can guide the deployment of weather-specific road safety measures in areas prone to severe weather-related accidents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Assuming 'df' is your PySpark DataFrame and 'Sex_of_driver' is the column of interest\n",
    "df_filter = df.filter(df['Weather_conditions'].isNotNull() & (df['Weather_conditions'] != 'Other') & (df['Weather_conditions'] != 'Unknown'))\n",
    "df_filter = df_filter.filter(df_filter['Accident_severity'].isNotNull() & (df_filter['Accident_severity'] != 'unknown') & (df_filter['Accident_severity'] != 'Unknown'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from scipy.stats import chi2_contingency\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"ChiSquareTest\").getOrCreate()\n",
    "\n",
    "# Assuming df is your PySpark DataFrame\n",
    "# Create the contingency table\n",
    "contingency_table = df.groupBy('Weather_conditions', 'Accident_severity').count()\n",
    "\n",
    "# Convert to Pandas DataFrame for chi-square test\n",
    "contingency_pd = contingency_table.toPandas()\n",
    "contingency_pd_pivot = contingency_pd.pivot(index='Weather_conditions', columns='Accident_severity', values='count').fillna(0)\n",
    "\n",
    "# Perform the Chi-Square Test\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_pd_pivot)\n",
    "\n",
    "# Output the results\n",
    "print(chi2, p, dof, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Light_conditions vs Accident_severity*\n",
    "Understanding how light conditions affect accident severity can inform infrastructure improvements (like street lighting) in specific areas, and help in scheduling increased patrolling during high-risk light conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Assuming 'df' is your PySpark DataFrame and 'Sex_of_driver' is the column of interest\n",
    "df_filter = df\n",
    "df_filter = df_filter.filter(df_filter['Accident_severity'].isNotNull() & (df_filter['Accident_severity'] != 'unknown') & (df_filter['Accident_severity'] != 'Unknown'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from scipy.stats import chi2_contingency\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"ChiSquareTest\").getOrCreate()\n",
    "\n",
    "# Assuming df is your PySpark DataFrame\n",
    "# Create the contingency table\n",
    "contingency_table = df.groupBy('Light_conditions', 'Accident_severity').count()\n",
    "\n",
    "# Convert to Pandas DataFrame for chi-square test\n",
    "contingency_pd = contingency_table.toPandas()\n",
    "contingency_pd_pivot = contingency_pd.pivot(index='Light_conditions', columns='Accident_severity', values='count').fillna(0)\n",
    "\n",
    "# Perform the Chi-Square Test\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_pd_pivot)\n",
    "\n",
    "# Output the results\n",
    "print(chi2, p, dof, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Road_surface_conditions vs Accident_severity*\n",
    "Knowing how road conditions influence accident severity can guide maintenance and infrastructure development in specific geographic locations to improve road safety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Assuming 'df' is your PySpark DataFrame and 'Sex_of_driver' is the column of interest\n",
    "df_filter = df\n",
    "df_filter = df_filter.filter(df_filter['Accident_severity'].isNotNull() & (df_filter['Accident_severity'] != 'unknown') & (df_filter['Accident_severity'] != 'Unknown'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from scipy.stats import chi2_contingency\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"ChiSquareTest\").getOrCreate()\n",
    "\n",
    "# Assuming df is your PySpark DataFrame\n",
    "# Create the contingency table\n",
    "contingency_table = df.groupBy('Road_surface_conditions', 'Accident_severity').count()\n",
    "\n",
    "# Convert to Pandas DataFrame for chi-square test\n",
    "contingency_pd = contingency_table.toPandas()\n",
    "contingency_pd_pivot = contingency_pd.pivot(index='Road_surface_conditions', columns='Accident_severity', values='count').fillna(0)\n",
    "\n",
    "# Perform the Chi-Square Test\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_pd_pivot)\n",
    "\n",
    "# Output the results\n",
    "print(chi2, p, dof, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Type_of_vehicle vs Accident_severity*\n",
    "This correlation can inform targeted vehicle safety standards or advisories for certain vehicle types, especially in areas where these vehicles are involved in more severe accidents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'df' is your PySpark DataFrame and 'Sex_of_driver' is the column of interest\n",
    "df_filter = df.filter(df['Type_of_vehicle'].isNotNull() & (df['Type_of_vehicle'] != 'Other') & (df['Type_of_vehicle'] != 'Bajaj'))\n",
    "# Assuming df_filter is your PySpark DataFrame\n",
    "df_filter = df_filter.withColumn(\"Type_of_vehicle\", F.regexp_replace(F.col(\"Type_of_vehicle\"), r\"\\s\\(.*\\)\", \"\"))\n",
    "\n",
    "df_filter = df_filter.filter(df_filter['Accident_severity'].isNotNull() & (df_filter['Accident_severity'] != 'unknown') & (df_filter['Accident_severity'] != 'Unknown'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from scipy.stats import chi2_contingency\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"ChiSquareTest\").getOrCreate()\n",
    "\n",
    "# Assuming df is your PySpark DataFrame\n",
    "# Create the contingency table\n",
    "contingency_table = df.groupBy('Type_of_vehicle', 'Accident_severity').count()\n",
    "\n",
    "# Convert to Pandas DataFrame for chi-square test\n",
    "contingency_pd = contingency_table.toPandas()\n",
    "contingency_pd_pivot = contingency_pd.pivot(index='Type_of_vehicle', columns='Accident_severity', values='count').fillna(0)\n",
    "\n",
    "# Perform the Chi-Square Test\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_pd_pivot)\n",
    "\n",
    "# Output the results\n",
    "print(chi2, p, dof, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Area_accident_occurred vs Accident_severity*\n",
    "Identifying areas prone to severe accidents is directly related to your goal. This information can be used to focus road safety improvements, traffic regulation changes, or public awareness campaigns in these specific locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Assuming 'df' is your PySpark DataFrame and 'Sex_of_driver' is the column of interest\n",
    "df_filter = df.filter(df['Area_accident_occured'].isNotNull() & (df['Area_accident_occured'] != 'Other') & (df['Area_accident_occured'] != 'Unknown') & (df['Area_accident_occured'] != 'Rural village areasOffice areas'))\n",
    "df_filter = df_filter.filter(df_filter['Accident_severity'].isNotNull() & (df_filter['Accident_severity'] != 'unknown') & (df_filter['Accident_severity'] != 'Unknown'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from scipy.stats import chi2_contingency\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"ChiSquareTest\").getOrCreate()\n",
    "\n",
    "# Assuming df is your PySpark DataFrame\n",
    "# Create the contingency table\n",
    "contingency_table = df.groupBy('Area_accident_occured', 'Accident_severity').count()\n",
    "\n",
    "# Convert to Pandas DataFrame for chi-square test\n",
    "contingency_pd = contingency_table.toPandas()\n",
    "contingency_pd_pivot = contingency_pd.pivot(index='Area_accident_occured', columns='Accident_severity', values='count').fillna(0)\n",
    "\n",
    "# Perform the Chi-Square Test\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_pd_pivot)\n",
    "\n",
    "# Output the results\n",
    "print(chi2, p, dof, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Analysis - Visual Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap for Temporal Pattern\n",
    "heatmaps for time-related data like \n",
    "\n",
    "*'Day_of_week' vs 'accident frequency'*\n",
    "\n",
    "*'Hour'vs 'accident frequency'*\n",
    "\n",
    "time of an accident is a significant factor and should definitely be included in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import hour, minute\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = df.withColumn('Hour', hour(df['Time']))\n",
    "df = df.withColumn('Minute', minute(df['Time']))\n",
    "\n",
    "df_filter = df.filter(df['Day_of_week'].isNotNull())\n",
    "df_filter = df_filter.filter(df_filter['Hour'].isNotNull())\n",
    "df_filter = df_filter.filter(df_filter['Accident_severity'].isNotNull() & (df_filter['Accident_severity'] != 'unknown') & (df_filter['Accident_severity'] != 'Unknown'))\n",
    "\n",
    "# Get distinct values for 'Day_of_week' and 'Area_accident_occured'\n",
    "day_of_week_unique = df_filter.select('Day_of_week').distinct()\n",
    "area_accident_occured_unique = df_filter.select('Area_accident_occured').distinct()\n",
    "\n",
    "# Creating a crosstab (contingency table) in PySpark\n",
    "contingency_table_df = df_filter.crosstab('Day_of_week', 'Area_accident_occured')\n",
    "\n",
    "# Convert the PySpark DataFrame to Pandas DataFrame for plotting\n",
    "contingency_table_pd = contingency_table_df.toPandas()\n",
    "\n",
    "# Set an appropriate index for the heatmap\n",
    "contingency_table_pd = contingency_table_pd.set_index('Day_of_week_Area_accident_occured')\n",
    "\n",
    "# Plotting the heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(contingency_table_pd, annot=True, fmt='d', cmap='viridis')\n",
    "plt.title('Correlation between Area of Accident Occurrence and Day of the Week')\n",
    "plt.ylabel('Day of the Week')\n",
    "plt.xlabel('Area of Accident Occurrence')\n",
    "plt.show()\n",
    "\n",
    "# Get distinct values for 'Day_of_week' and 'Area_accident_occured'\n",
    "Hour_unique = df_filter.select('Hour').distinct()\n",
    "area_accident_occured_unique = df_filter.select('Area_accident_occured').distinct()\n",
    "\n",
    "# Creating a crosstab (contingency table) in PySpark\n",
    "contingency_table_df = df_filter.crosstab('Hour', 'Area_accident_occured')\n",
    "\n",
    "# Convert the PySpark DataFrame to Pandas DataFrame for plotting\n",
    "contingency_table_pd = contingency_table_df.toPandas()\n",
    "\n",
    "# Set an appropriate index for the heatmap\n",
    "contingency_table_pd = contingency_table_pd.set_index('Hour_Area_accident_occured')\n",
    "\n",
    "# Plotting the heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(contingency_table_pd, annot=True, fmt='d', cmap='viridis')\n",
    "plt.title('Correlation between Area of Accident Occurrence and Hours')\n",
    "plt.ylabel('Hour')\n",
    "plt.xlabel('Area of Accident Occurrence')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bar Charts for Categorical Data Analysis\n",
    "\n",
    "*'Weather_conditions' vs 'count of accidents'*: even if accidents are less frequent in adverse weather conditions, they might still be more severe, so this feature should be included in the model\n",
    "\n",
    "*'Road_surface_conditions' vs 'count of accidents'*:  the severity and the risk of accidents might increase with adverse road conditions\n",
    "\n",
    "*'Light_conditions' vs 'count of accidents'*: show a strong correlation with the number of accidents\n",
    "\n",
    "*'Type_of_vehicle' vs 'count of accidents'*: This feature can help in understanding if certain vehicle types are more prone to accidents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Assuming 'df' is your PySpark DataFrame and 'Sex_of_driver' is the column of interest\n",
    "df_filter = df.filter(df['Weather_conditions'].isNotNull() & (df['Weather_conditions'] != 'Other') & (df['Weather_conditions'] != 'Unknown'))\n",
    "df_filter = df_filter.filter(df_filter['Road_surface_conditions'].isNotNull())\n",
    "df_filter = df_filter.filter(df_filter['Light_conditions'].isNotNull())\n",
    "df_filter = df_filter.filter(df_filter['Type_of_vehicle'].isNotNull() & (df['Type_of_vehicle'] != 'Other') & (df['Type_of_vehicle'] != 'Bajaj'))\n",
    "df_filter = df_filter.withColumn(\"Type_of_vehicle\", F.regexp_replace(F.col(\"Type_of_vehicle\"), r\"\\s\\(.*\\)\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming df_filter is your PySpark DataFrame\n",
    "# Group by 'Weather_conditions' and count\n",
    "accident_counts = df_filter.groupBy('Weather_conditions').count()\n",
    "\n",
    "# Convert to Pandas DataFrame for plotting\n",
    "accident_counts_pd = accident_counts.toPandas()\n",
    "\n",
    "# Sort the DataFrame for better visualization\n",
    "accident_counts_pd = accident_counts_pd.sort_values(by='count', ascending=False)\n",
    "\n",
    "# Creating the bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Weather_conditions', y='count', data=accident_counts_pd)\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title('Count of Accidents by Weather Conditions')\n",
    "plt.xlabel('Weather Conditions')\n",
    "plt.ylabel('Number of Accidents')\n",
    "plt.xticks(rotation=45)  # Rotating the x labels for better readability\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "# Group by 'Road_surface_conditions' and count\n",
    "road_surface_counts = df_filter.groupBy('Road_surface_conditions').count()\n",
    "\n",
    "# Convert to Pandas DataFrame for plotting\n",
    "road_surface_counts_pd = road_surface_counts.toPandas().sort_values(by='count', ascending=False)\n",
    "\n",
    "# Creating the bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Road_surface_conditions', y='count', data=road_surface_counts_pd)\n",
    "plt.title('Count of Accidents by Road Surface Conditions')\n",
    "plt.xlabel('Road Surface Conditions')\n",
    "plt.ylabel('Number of Accidents')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Group by 'Light_conditions' and count\n",
    "light_conditions_counts = df_filter.groupBy('Light_conditions').count()\n",
    "\n",
    "# Convert to Pandas DataFrame for plotting\n",
    "light_conditions_counts_pd = light_conditions_counts.toPandas().sort_values(by='count', ascending=False)\n",
    "\n",
    "# Creating the bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Light_conditions', y='count', data=light_conditions_counts_pd)\n",
    "plt.title('Count of Accidents by Light Conditions')\n",
    "plt.xlabel('Light Conditions')\n",
    "plt.ylabel('Number of Accidents')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Group by 'Type_of_vehicle' and count\n",
    "type_of_vehicle_counts = df_filter.groupBy('Type_of_vehicle').count()\n",
    "\n",
    "# Convert to Pandas DataFrame for plotting\n",
    "type_of_vehicle_counts_pd = type_of_vehicle_counts.toPandas().sort_values(by='count', ascending=False)\n",
    "\n",
    "# Creating the bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Type_of_vehicle', y='count', data=type_of_vehicle_counts_pd)\n",
    "plt.title('Count of Accidents by Type of Vehicle')\n",
    "plt.xlabel('Type of Vehicle')\n",
    "plt.ylabel('Number of Accidents')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import hour\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df_filter = df.withColumn('Hour', hour(df['Time']))\n",
    "df_filter = df_filter.filter(df['Hour'].isNotNull())\n",
    "df_filter = df_filter.filter(df['Day_of_week'].isNotNull())\n",
    "\n",
    "# Group the data by 'Day_of_week' and 'Hour' and count the number of accidents\n",
    "accidents_by_day_hour = df_filter.groupBy('Day_of_week', 'Hour').count()\n",
    "\n",
    "# Convert to Pandas DataFrame for plotting\n",
    "accidents_by_day_hour_pd = accidents_by_day_hour.toPandas()\n",
    "\n",
    "# Pivot the data for better visualization\n",
    "accidents_pivot = accidents_by_day_hour_pd.pivot(index='Hour', columns='Day_of_week', values='count').fillna(0)\n",
    "\n",
    "# Plotting the data\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(accidents_pivot, annot=False, fmt=\".0f\", linewidths=.5, cmap='viridis')\n",
    "plt.title('Distribution of Accidents by Day of Week and Hour')\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('Hour of Day')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avoiding Multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+\n",
      "|               Time|Hour|\n",
      "+-------------------+----+\n",
      "|2023-12-11 17:02:00|  17|\n",
      "|2023-12-11 17:02:00|  17|\n",
      "|2023-12-11 17:02:00|  17|\n",
      "|2023-12-11 01:06:00|   1|\n",
      "|2023-12-11 01:06:00|   1|\n",
      "|2023-12-11 14:15:00|  14|\n",
      "|2023-12-11 17:30:00|  17|\n",
      "|2023-12-11 17:20:00|  17|\n",
      "|2023-12-11 17:20:00|  17|\n",
      "|2023-12-11 17:20:00|  17|\n",
      "|2023-12-11 14:40:00|  14|\n",
      "|2023-12-11 14:40:00|  14|\n",
      "|2023-12-11 17:45:00|  17|\n",
      "|2023-12-11 17:45:00|  17|\n",
      "|2023-12-11 17:45:00|  17|\n",
      "|2023-12-11 22:45:00|  22|\n",
      "|2023-12-11 22:45:00|  22|\n",
      "|2023-12-11 22:45:00|  22|\n",
      "|2023-12-11 22:45:00|  22|\n",
      "|2023-12-11 08:20:00|   8|\n",
      "+-------------------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------+-------------------+\n",
      "|Day_of_week|Day_of_week_numeric|\n",
      "+-----------+-------------------+\n",
      "|     Monday|                  1|\n",
      "|     Monday|                  1|\n",
      "|     Monday|                  1|\n",
      "|     Sunday|                  7|\n",
      "|     Sunday|                  7|\n",
      "|     Friday|                  5|\n",
      "|  Wednesday|                  3|\n",
      "|     Friday|                  5|\n",
      "|     Friday|                  5|\n",
      "|     Friday|                  5|\n",
      "|   Saturday|                  6|\n",
      "|   Saturday|                  6|\n",
      "|   Thursday|                  4|\n",
      "|   Thursday|                  4|\n",
      "|   Thursday|                  4|\n",
      "|     Monday|                  1|\n",
      "|     Monday|                  1|\n",
      "|     Monday|                  1|\n",
      "|     Monday|                  1|\n",
      "|    Tuesday|                  2|\n",
      "+-----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/11 01:14:46 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 10:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+------------------+-------------+------------------+-----------------------+------------------+--------------------+----------------+-----------------------+-----------------+---------------------+--------------------+--------------------+-----------------+-----------------+-----------------------+--------------------+------------------+--------------------+---------------------------+--------------------+----------------+---------------+---------------+--------------------+-----------------+-----------------+--------------------+--------------------+--------------------+-----------------+-------------------+----+\n",
      "|               Time|Day_of_week|Age_band_of_driver|Sex_of_driver| Educational_level|Vehicle_driver_relation|Driving_experience|     Type_of_vehicle|Owner_of_vehicle|Service_year_of_vehicle|Defect_of_vehicle|Area_accident_occured|    Lanes_or_Medians|     Road_allignment|Types_of_Junction|Road_surface_type|Road_surface_conditions|    Light_conditions|Weather_conditions|   Type_of_collision|Number_of_vehicles_involved|Number_of_casualties|Vehicle_movement| Casualty_class|Sex_of_casualty|Age_band_of_casualty|Casualty_severity|Work_of_casuality|Fitness_of_casuality| Pedestrian_movement|   Cause_of_accident|Accident_severity|Day_of_week_numeric|Hour|\n",
      "+-------------------+-----------+------------------+-------------+------------------+-----------------------+------------------+--------------------+----------------+-----------------------+-----------------+---------------------+--------------------+--------------------+-----------------+-----------------+-----------------------+--------------------+------------------+--------------------+---------------------------+--------------------+----------------+---------------+---------------+--------------------+-----------------+-----------------+--------------------+--------------------+--------------------+-----------------+-------------------+----+\n",
      "|2023-12-11 17:02:00|     Monday|             18-30|         Male| Above high school|               Employee|             1-2yr|          Automobile|           Owner|             Above 10yr|        No defect|    Residential areas|                NULL|Tangent road with...|      No junction|    Asphalt roads|                    Dry|            Daylight|            Normal|Collision with ro...|                          2|                   2|  Going straight|             na|             na|                  na|               na|             NULL|                NULL|    Not a Pedestrian|     Moving Backward|    Slight Injury|                  1|  17|\n",
      "|2023-12-11 17:02:00|     Monday|             31-50|         Male|Junior high school|               Employee|        Above 10yr| Public (> 45 seats)|           Owner|                5-10yrs|        No defect|         Office areas|   Undivided Two way|Tangent road with...|      No junction|    Asphalt roads|                    Dry|            Daylight|            Normal|Vehicle with vehi...|                          2|                   2|  Going straight|             na|             na|                  na|               na|             NULL|                NULL|    Not a Pedestrian|          Overtaking|    Slight Injury|                  1|  17|\n",
      "|2023-12-11 17:02:00|     Monday|             18-30|         Male|Junior high school|               Employee|             1-2yr|     Lorry (41?100Q)|           Owner|                   NULL|        No defect|   Recreational areas|               other|                NULL|      No junction|    Asphalt roads|                    Dry|            Daylight|            Normal|Collision with ro...|                          2|                   2|  Going straight|Driver or rider|           Male|               31-50|                3|           Driver|                NULL|    Not a Pedestrian|Changing lane to ...|   Serious Injury|                  1|  17|\n",
      "|2023-12-11 01:06:00|     Sunday|             18-30|         Male|Junior high school|               Employee|            5-10yr| Public (> 45 seats)|    Governmental|                   NULL|        No defect|         Office areas|               other|Tangent road with...|          Y Shape|      Earth roads|                    Dry|Darkness - lights...|            Normal|Vehicle with vehi...|                          2|                   2|  Going straight|     Pedestrian|         Female|               18-30|                3|           Driver|              Normal|    Not a Pedestrian|Changing lane to ...|    Slight Injury|                  7|   1|\n",
      "|2023-12-11 01:06:00|     Sunday|             18-30|         Male|Junior high school|               Employee|             2-5yr|                NULL|           Owner|                5-10yrs|        No defect|     Industrial areas|               other|Tangent road with...|          Y Shape|    Asphalt roads|                    Dry|Darkness - lights...|            Normal|Vehicle with vehi...|                          2|                   2|  Going straight|             na|             na|                  na|               na|             NULL|                NULL|    Not a Pedestrian|          Overtaking|    Slight Injury|                  7|   1|\n",
      "|2023-12-11 14:15:00|     Friday|             31-50|         Male|              NULL|                Unknown|              NULL|                NULL|            NULL|                   NULL|             NULL|                 NULL|                NULL|                NULL|          Y Shape|             NULL|                    Dry|            Daylight|            Normal|Vehicle with vehi...|                          1|                   1|          U-Turn|Driver or rider|           Male|               31-50|                3|           Driver|              Normal|    Not a Pedestrian|         Overloading|    Slight Injury|                  5|  14|\n",
      "|2023-12-11 17:30:00|  Wednesday|             18-30|         Male|Junior high school|               Employee|             2-5yr|          Automobile|           Owner|                   NULL|        No defect|    Residential areas|   Undivided Two way|Tangent road with...|         Crossing|             NULL|                    Dry|            Daylight|            Normal|Vehicle with vehi...|                          1|                   1| Moving Backward|Driver or rider|         Female|               18-30|                3|           Driver|              Normal|    Not a Pedestrian|               Other|    Slight Injury|                  3|  17|\n",
      "|2023-12-11 17:20:00|     Friday|             18-30|         Male|Junior high school|               Employee|             2-5yr|          Automobile|    Governmental|             Above 10yr|        No defect|    Residential areas|               other|Tangent road with...|          Y Shape|    Asphalt roads|                    Dry|            Daylight|            Normal|Vehicle with vehi...|                          2|                   1|          U-Turn|             na|             na|                  na|               na|             NULL|              Normal|    Not a Pedestrian|No priority to ve...|    Slight Injury|                  5|  17|\n",
      "|2023-12-11 17:20:00|     Friday|             18-30|         Male|Junior high school|               Employee|        Above 10yr|     Lorry (41?100Q)|           Owner|                  1-2yr|        No defect|     Industrial areas|               other|Tangent road with...|          Y Shape|      Earth roads|                    Dry|            Daylight|            Normal|Collision with ro...|                          2|                   1|  Going straight|     Pedestrian|           Male|            Under 18|                3|           Driver|              Normal|Crossing from dri...|Changing lane to ...|    Slight Injury|                  5|  17|\n",
      "|2023-12-11 17:20:00|     Friday|             18-30|         Male|Junior high school|               Employee|             1-2yr|          Automobile|           Owner|                 2-5yrs|        No defect|    Residential areas|   Undivided Two way|Tangent road with...|          Y Shape|    Asphalt roads|                    Dry|            Daylight|            Normal|Collision with ro...|                          2|                   1|          U-Turn|      Passenger|           Male|               18-30|                3|           Driver|              Normal|    Not a Pedestrian|     Moving Backward|   Serious Injury|                  5|  17|\n",
      "|2023-12-11 14:40:00|   Saturday|             18-30|         Male| Above high school|                  Owner|             1-2yr|Public (13?45 seats)|           Owner|                Unknown|        No defect|    Residential areas|               other|Tangent road with...|      No junction|    Asphalt roads|                    Dry|            Daylight|            Normal|Collision with an...|                          2|                   1|        Turnover|             na|             na|                  na|               na|             NULL|              Normal|    Not a Pedestrian|Changing lane to ...|   Serious Injury|                  6|  14|\n",
      "|2023-12-11 14:40:00|   Saturday|             31-50|         Male| Above high school|               Employee|        No Licence|          Automobile|           Owner|                   NULL|        No defect|         Office areas|   Undivided Two way|Tangent road with...|      No junction|      Earth roads|                    Dry|            Daylight|            Normal|Collision with an...|                          2|                   1|  Going straight|Driver or rider|           Male|               18-30|                3|           Driver|              Normal|    Not a Pedestrian|No priority to pe...|   Serious Injury|                  6|  14|\n",
      "|2023-12-11 17:45:00|   Thursday|             18-30|         Male|Junior high school|               Employee|             1-2yr| Public (> 45 seats)|           Owner|                 2-5yrs|        No defect|         Office areas|Double carriagewa...|         Escarpments|      No junction|    Asphalt roads|                    Dry|            Daylight|            Normal|Collision with an...|                          2|                   2|  Going straight|             na|             na|                  na|               na|           Driver|              Normal|    Not a Pedestrian|       No distancing|    Slight Injury|                  4|  17|\n",
      "|2023-12-11 17:45:00|   Thursday|             31-50|         Male|Junior high school|               Employee|            5-10yr|     Lorry (41?100Q)|           Owner|             Above 10yr|        No defect|         Office areas|               other|Tangent road with...|      No junction|    Asphalt roads|                    Dry|            Daylight|            Normal|Collision with an...|                          2|                   2|   Waiting to go|             na|             na|                  na|               na|            Other|              Normal|    Not a Pedestrian|No priority to ve...|    Slight Injury|                  4|  17|\n",
      "|2023-12-11 17:45:00|   Thursday|             31-50|         Male|Junior high school|               Employee|        Above 10yr|          Automobile|           Owner|                  1-2yr|        No defect|         Office areas|   Undivided Two way|         Escarpments|      No junction|    Asphalt roads|                    Dry|            Daylight|            Normal|Collision with an...|                          2|                   2|  Going straight|Driver or rider|         Female|               18-30|                3|           Driver|              Normal|    Not a Pedestrian|       No distancing|   Serious Injury|                  4|  17|\n",
      "|2023-12-11 22:45:00|     Monday|             18-30|       Female|Junior high school|               Employee|        Above 10yr|      Lorry (11?40Q)|           Owner|                   NULL|        No defect|         Office areas|             One way|Tangent road with...|          Y Shape|    Asphalt roads|            Wet or damp|Darkness - lights...|           Raining|Collision with an...|                          2|                   3|  Going straight|             na|             na|                  na|               na|             NULL|                NULL|    Not a Pedestrian|       No distancing|   Serious Injury|                  1|  22|\n",
      "|2023-12-11 22:45:00|     Monday|             18-30|         Male|Junior high school|                  Owner|            5-10yr|Public (13?45 seats)|           Owner|                   NULL|        No defect|         Office areas|   Undivided Two way|Tangent road with...|          Y Shape|    Asphalt roads|            Wet or damp|Darkness - lights...|           Raining|Collision with an...|                          2|                   3|  Going straight|             na|             na|                  na|               na|           Driver|              Normal|    Not a Pedestrian|     Moving Backward|    Slight Injury|                  1|  22|\n",
      "|2023-12-11 22:45:00|     Monday|             18-30|         Male| Elementary school|               Employee|             2-5yr|Public (13?45 seats)|           Owner|                   NULL|        No defect|         Office areas|               other|Tangent road with...|          Y Shape|    Asphalt roads|            Wet or damp|Darkness - lights...|           Raining|Collision with an...|                          2|                   3|  Going straight|             na|             na|                  na|               na|           Driver|              Normal|    Not a Pedestrian|Changing lane to ...|   Serious Injury|                  1|  22|\n",
      "|2023-12-11 22:45:00|     Monday|             18-30|         Male|Junior high school|               Employee|             2-5yr|                NULL|           Owner|             Above 10yr|        No defect|    Residential areas|               other|Tangent road with...|          Y Shape|    Asphalt roads|            Wet or damp|Darkness - lights...|           Raining|Collision with an...|                          2|                   3| Moving Backward|             na|             na|                  na|               na|       Unemployed|              Normal|    Not a Pedestrian|No priority to pe...|    Slight Injury|                  1|  22|\n",
      "|2023-12-11 08:20:00|    Tuesday|             18-30|         Male|Junior high school|               Employee|         Below 1yr|          Long lorry|           Owner|                5-10yrs|        No defect|    Residential areas|             One way|Tangent road with...|          Y Shape|    Asphalt roads|                    Dry|            Daylight|            Normal|Collision with an...|                          2|                   1|  Going straight|Driver or rider|           Male|            Under 18|                3|           Driver|              Normal|    Not a Pedestrian|     Moving Backward|   Serious Injury|                  2|   8|\n",
      "+-------------------+-----------+------------------+-------------+------------------+-----------------------+------------------+--------------------+----------------+-----------------------+-----------------+---------------------+--------------------+--------------------+-----------------+-----------------+-----------------------+--------------------+------------------+--------------------+---------------------------+--------------------+----------------+---------------+---------------+--------------------+-----------------+-----------------+--------------------+--------------------+--------------------+-----------------+-------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import hour\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Accident Hotspots Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load your DataFrame here\n",
    "df = spark.read.csv('Road.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Create a user-defined function for converting days to numbers\n",
    "days_dict = {'Monday': 1, 'Tuesday': 2, 'Wednesday': 3, 'Thursday': 4, 'Friday': 5, 'Saturday': 6, 'Sunday': 7}\n",
    "convert_day_udf = udf(lambda day: days_dict.get(day, 0), IntegerType())\n",
    "\n",
    "# Apply the UDF to the 'Day_of_week' column\n",
    "df = df.withColumn('Day_of_week_numeric', convert_day_udf(df['Day_of_week']))\n",
    "# Extract the hour from the 'Time' column\n",
    "df = df.withColumn(\"Hour\", hour(\"Time\"))\n",
    "\n",
    "# Check the transformation\n",
    "df.select('Time', 'Hour').show()\n",
    "\n",
    "# Check the transformation\n",
    "df.select('Day_of_week', 'Day_of_week_numeric').show()\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "# Add an index column to the original DataFrame\n",
    "df = df.withColumn(\"index\", monotonically_increasing_id())\n",
    "\n",
    "# Filter out rows with null or 'Unknown' values in 'Weather_conditions'\n",
    "df_filtered = df.filter((df.Weather_conditions.isNotNull()) & (df.Weather_conditions != 'Unknown'))\n",
    "\n",
    "# StringIndexer to convert the 'Weather_conditions' column to numeric indices\n",
    "stringIndexer = StringIndexer(inputCol=\"Weather_conditions\", outputCol=\"Weather_conditions_Index\")\n",
    "\n",
    "# Fit the indexer to the data and transform it\n",
    "indexed = stringIndexer.fit(df).transform(df_filtered)\n",
    "\n",
    "# OneHotEncoder to convert indexed numbers to one-hot encoded values\n",
    "encoder = OneHotEncoder(inputCols=[\"Weather_conditions_Index\"], outputCols=[\"Weather_conditions_Encoded\"])\n",
    "\n",
    "# Fit and transform\n",
    "encoded = encoder.fit(indexed).transform(indexed)\n",
    "\n",
    "# Add the same index column to the encoded DataFrame\n",
    "encoded = encoded.withColumn(\"index\", monotonically_increasing_id())\n",
    "\n",
    "# Join using the index\n",
    "df = df.join(encoded.select(\"index\", \"Weather_conditions_Encoded\"), on=[\"index\"])\n",
    "\n",
    "# Optionally, you can drop the index column after joining if it's no longer needed\n",
    "df = df.drop(\"index\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "# Add an index column to the original DataFrame\n",
    "df = df.withColumn(\"index\", monotonically_increasing_id())\n",
    "\n",
    "# Filter out rows with null or 'Unknown' values in 'Weather_conditions'\n",
    "df_filtered = df.filter((df.Light_conditions.isNotNull()) & (df.Light_conditions != 'Unknown'))\n",
    "\n",
    "# StringIndexer to convert the 'Light_conditions' column to numeric indices\n",
    "stringIndexer = StringIndexer(inputCol=\"Light_conditions\", outputCol=\"Light_conditions_Index\")\n",
    "\n",
    "# Fit the indexer to the data and transform it\n",
    "indexed = stringIndexer.fit(df).transform(df_filtered)\n",
    "\n",
    "# OneHotEncoder to convert indexed numbers to one-hot encoded values\n",
    "encoder = OneHotEncoder(inputCols=[\"Light_conditions_Index\"], outputCols=[\"Light_conditions_Encoded\"])\n",
    "\n",
    "# Fit and transform\n",
    "encoded = encoder.fit(indexed).transform(indexed)\n",
    "\n",
    "# Add the same index column to the encoded DataFrame\n",
    "encoded = encoded.withColumn(\"index\", monotonically_increasing_id())\n",
    "\n",
    "# Join using the index\n",
    "df = df.join(encoded.select(\"index\", \"Light_conditions_Encoded\"), on=[\"index\"])\n",
    "\n",
    "# Optionally, you can drop the index column after joining if it's no longer needed\n",
    "df = df.drop(\"index\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "# Add an index column to the original DataFrame\n",
    "df = df.withColumn(\"index\", monotonically_increasing_id())\n",
    "\n",
    "# Filter out rows with null or 'Unknown' values in 'Weather_conditions'\n",
    "df_filtered = df.filter((df.Road_surface_conditions.isNotNull()) & (df.Road_surface_conditions != 'Unknown'))\n",
    "\n",
    "# StringIndexer to convert the 'Light_conditions' column to numeric indices\n",
    "stringIndexer = StringIndexer(inputCol=\"Road_surface_conditions\", outputCol=\"Road_surface_conditions_Index\")\n",
    "\n",
    "# Fit the indexer to the data and transform it\n",
    "indexed = stringIndexer.fit(df).transform(df_filtered)\n",
    "\n",
    "# OneHotEncoder to convert indexed numbers to one-hot encoded values\n",
    "encoder = OneHotEncoder(inputCols=[\"Road_surface_conditions_Index\"], outputCols=[\"Road_surface_conditions_Encoded\"])\n",
    "\n",
    "# Fit and transform\n",
    "encoded = encoder.fit(indexed).transform(indexed)\n",
    "\n",
    "# Add the same index column to the encoded DataFrame\n",
    "encoded = encoded.withColumn(\"index\", monotonically_increasing_id())\n",
    "\n",
    "# Join using the index\n",
    "df = df.join(encoded.select(\"index\", \"Road_surface_conditions_Encoded\"), on=[\"index\"])\n",
    "\n",
    "# Optionally, you can drop the index column after joining if it's no longer needed\n",
    "df = df.drop(\"index\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = df.drop(\"Type_of_vehicle_Encoded\")\n",
    "\n",
    "# Add an index column to the original DataFrame\n",
    "df = df.withColumn(\"index\", monotonically_increasing_id())\n",
    "\n",
    "# Filter out unwanted rows and clean up Type_of_vehicle column\n",
    "df = df.filter(df['Type_of_vehicle'].isNotNull() & (df['Type_of_vehicle'] != 'Other') & (df['Type_of_vehicle'] != 'Bajaj'))\n",
    "df = df.withColumn(\"Type_of_vehicle\", F.regexp_replace(F.col(\"Type_of_vehicle\"), r\"\\s\\(.*\\)\", \"\"))\n",
    "\n",
    "# Apply StringIndexer with handleInvalid set to 'keep'\n",
    "stringIndexer = StringIndexer(inputCol=\"Type_of_vehicle\", outputCol=\"Type_of_vehicle_Index\", handleInvalid=\"keep\")\n",
    "indexed = stringIndexer.fit(df).transform(df)\n",
    "\n",
    "# Apply OneHotEncoder\n",
    "encoder = OneHotEncoder(inputCols=[\"Type_of_vehicle_Index\"], outputCols=[\"Type_of_vehicle_Encoded\"])\n",
    "encoded = encoder.fit(indexed).transform(indexed)\n",
    "\n",
    "# Join using the index\n",
    "df = df.join(encoded.select(\"index\", \"Type_of_vehicle_Encoded\"), on=[\"index\"])\n",
    "\n",
    "# Optionally, you can drop the index column after joining if it's no longer needed\n",
    "df = df.drop(\"index\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "df = df.drop(\"Area_accident_occured_Encoded\")\n",
    "\n",
    "# Add an index column to the original DataFrame\n",
    "df = df.withColumn(\"index\", monotonically_increasing_id())\n",
    "\n",
    "# Filter out unwanted rows and clean up Type_of_vehicle column\n",
    "df = df.filter(df['Area_accident_occured'].isNotNull() & (df['Area_accident_occured'] != 'Other') & (df['Area_accident_occured'] != 'Unknown') & (df['Area_accident_occured'] != 'Rural village areasOffice areas'))\n",
    "\n",
    "# Apply StringIndexer with handleInvalid set to 'keep'\n",
    "stringIndexer = StringIndexer(inputCol=\"Area_accident_occured\", outputCol=\"Area_accident_occured_Index\", handleInvalid=\"keep\")\n",
    "indexed = stringIndexer.fit(df).transform(df)\n",
    "\n",
    "# Apply OneHotEncoder\n",
    "encoder = OneHotEncoder(inputCols=[\"Area_accident_occured_Index\"], outputCols=[\"Area_accident_occured_Encoded\"])\n",
    "encoded = encoder.fit(indexed).transform(indexed)\n",
    "\n",
    "# Join using the index\n",
    "df = df.join(encoded.select(\"index\", \"Area_accident_occured_Encoded\"), on=[\"index\"])\n",
    "\n",
    "# Optionally, you can drop the index column after joining if it's no longer needed\n",
    "df = df.drop(\"index\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "if 'label' in df.columns:\n",
    "    df = df.drop('label')\n",
    "\n",
    "# Encode the label column if necessary\n",
    "labelIndexer = StringIndexer(inputCol=\"Accident_severity\", outputCol=\"label\", handleInvalid=\"keep\")\n",
    "df = labelIndexer.fit(df).transform(df)\n",
    "\n",
    "# Assemble the features\n",
    "assembler = VectorAssembler(inputCols=[\n",
    "    'Weather_conditions_Encoded', 'Light_conditions_Encoded', 'Road_surface_conditions_Encoded',\n",
    "    'Type_of_vehicle_Encoded', 'Area_accident_occured_Encoded', 'Day_of_week_numeric', 'Hour'\n",
    "], outputCol=\"features\")\n",
    "df_assembled = assembler.transform(df)\n",
    "\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train, test = df_assembled.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Create the RandomForestClassifier model\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=20, maxDepth=10, maxBins=32, minInstancesPerNode=2, seed=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "model = rf.fit(train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.transform(test)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Accuracy = %g\" % accuracy)\n",
    "\n",
    "# Show some predictions\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
